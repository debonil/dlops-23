{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrezbrBH6hGY",
        "outputId": "32287c1a-aea7-407d-d6a2-6bc135c68a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-tb-profiler in /usr/local/lib/python3.9/dist-packages (0.4.1)\n",
            "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /usr/local/lib/python3.9/dist-packages (from torch-tb-profiler) (2.12.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from torch-tb-profiler) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->torch-tb-profiler) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->torch-tb-profiler) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->torch-tb-profiler) (1.22.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (67.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.7.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.53.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.4.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.4.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.20.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.9/dist-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx) (4.5.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.9/dist-packages (1.14.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (23.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (23.3.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (1.11.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (1.22.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.9/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.9/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnxoptimizer in /usr/local/lib/python3.9/dist-packages (0.3.10)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.9/dist-packages (from onnxoptimizer) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxoptimizer) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxoptimizer) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx->onnxoptimizer) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-tb-profiler\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install onnxoptimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KasQYLJ6CegM",
        "outputId": "77165d79-6dba-4ce1-92c3-e88778a3665c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx.__version__='1.13.1', opset=18, IR_VERSION=8\n"
          ]
        }
      ],
      "source": [
        "from onnx import __version__, IR_VERSION\n",
        "from onnx.defs import onnx_opset_version\n",
        "print(\n",
        "    f\"onnx.__version__={__version__!r}, opset={onnx_opset_version()}, IR_VERSION={IR_VERSION}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1_cBvOk039jT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from time import perf_counter\n",
        "\n",
        "import torch.onnx\n",
        "import onnx\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import onnxruntime\n",
        "import os\n",
        "\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "\n",
        "def time_ort_model_evaluation(model_path):\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session = onnxruntime.InferenceSession(model_path, sess_options)\n",
        "\n",
        "    time_per_inference = []\n",
        "    for _ in range(10):\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        # compute ONNX Runtime output prediction\n",
        "        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
        "        start = perf_counter()\n",
        "        session.run(None, ort_inputs)\n",
        "        time_per_inference.append((1000 * (perf_counter() - start)))\n",
        "\n",
        "    return np.mean(time_per_inference)\n",
        "\n",
        "\n",
        "def time_ort_model_evaluation(model_path):\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session = onnxruntime.InferenceSession(model_path, sess_options)\n",
        "\n",
        "    time_per_inference = []\n",
        "    for _ in range(10):\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        # compute ONNX Runtime output prediction\n",
        "        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
        "        start = perf_counter()\n",
        "        session.run(None, ort_inputs)\n",
        "        time_per_inference.append((1000 * (perf_counter() - start)))\n",
        "\n",
        "    return np.mean(time_per_inference)\n",
        "\n",
        "\n",
        "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
        "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "    import onnx\n",
        "    onnx_opt_model = onnx.load(onnx_model_path)\n",
        "    quantize_dynamic(onnx_model_path,\n",
        "                     quantized_model_path,\n",
        "                     weight_type=QuantType.QUInt8)  # QInt8\n",
        "\n",
        "    print(f\"quantized model saved to:{quantized_model_path}\")\n",
        "\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "\n",
        "def perform_onnx_infer(model, model_name, dummy_input):\n",
        "\n",
        "    input_names = [\"actual_input\"]\n",
        "    output_names = [\"output\"]\n",
        "\n",
        "    print(\n",
        "        f'\\n\\n************************\\t{model_name}\\t********************************\\n\\n')\n",
        "\n",
        "    model_onnx = model_name + \".onnx\"\n",
        "    model_opt_onnx = model_name + \"_opt.onnx\"\n",
        "    model_opt_quant_onnx = model_name + \"_opt_quant.onnx\"\n",
        "\n",
        "    torch.onnx.export(model, dummy_input, model_onnx, verbose=False,\n",
        "                      input_names=input_names, output_names=output_names, export_params=True,)\n",
        "\n",
        "    ort_session = onnxruntime.InferenceSession(model_onnx)\n",
        "\n",
        "    # compute ONNX Runtime output prediction\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
        "    ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "    # compare ONNX Runtime and PyTorch results\n",
        "    torch_out = model(dummy_input)  # torch.randn(1, 3, 224, 224)\n",
        "    # np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "    os.system(f'!python -m onnxoptimizer {model_onnx} {model_opt_onnx}')\n",
        "\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Model in GPU: ' +\n",
        "          str(time_ort_model_evaluation(model_onnx)))\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Optimized Model in GPU: ' +\n",
        "          str(time_ort_model_evaluation(model_opt_onnx)))\n",
        "\n",
        "    quantize_onnx_model(model_opt_onnx, model_opt_quant_onnx)\n",
        "\n",
        "    print(f'{model_name}\\tONNX full precision model size (MB):',\n",
        "          os.path.getsize(model_opt_onnx)/(1024*1024))\n",
        "    print(f'{model_name}\\tONNX quantized model size (MB):', os.path.getsize(\n",
        "        model_opt_quant_onnx)/(1024*1024))\n",
        "\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Model in TPU: ' +\n",
        "          str(time_ort_model_evaluation(model_onnx)))\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Quantized Model in TPU: ' +\n",
        "          str(time_ort_model_evaluation(model_opt_quant_onnx)))\n",
        "    print()\n",
        "    print('-'*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XYSYS0F5t2N",
        "outputId": "d2d02d6b-489a-41b5-a5a4-084f81b60eca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "selected_models = [\n",
        "    torchvision.models.resnet34(pretrained=True).to(device),\n",
        "    torchvision.models.densenet121(pretrained=True).to(device),\n",
        "    torchvision.models.efficientnet_b0(pretrained=True).to(device),\n",
        "    torchvision.models.convnext_tiny(pretrained=True).to(device),\n",
        "]\n",
        "\n",
        "model_names = [\n",
        "    'ResNet-34', 'DenseNet-121', 'EfficientNet-B0', 'ConvNeXt-T'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI1wbXPO-qRd",
        "outputId": "a4085ef8-9c36-41b0-90bc-439e4560f8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "************************\tResNet-34\t********************************\n",
            "\n",
            "\n",
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "ResNet-34\tAverage runtime of ONNX Model in GPU: 101.58939989996725\n",
            "ResNet-34\tAverage runtime of ONNX Optimized Model in GPU: 99.03199939997194\n",
            "quantized model saved to:ResNet-34_opt_quant.onnx\n",
            "ResNet-34\tONNX full precision model size (MB): 83.13565731048584\n",
            "ResNet-34\tONNX quantized model size (MB): 20.88004970550537\n",
            "ResNet-34\tAverage runtime of ONNX Model in TPU: 147.2574743000223\n",
            "ResNet-34\tAverage runtime of ONNX Quantized Model in TPU: 176.98937220000062\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "************************\tDenseNet-121\t********************************\n",
            "\n",
            "\n",
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "DenseNet-121\tAverage runtime of ONNX Model in GPU: 85.70039960000031\n",
            "DenseNet-121\tAverage runtime of ONNX Optimized Model in GPU: 83.50030320002588\n",
            "quantized model saved to:DenseNet-121_opt_quant.onnx\n",
            "DenseNet-121\tONNX full precision model size (MB): 30.811330795288086\n",
            "DenseNet-121\tONNX quantized model size (MB): 8.444440841674805\n",
            "DenseNet-121\tAverage runtime of ONNX Model in TPU: 82.10658769994552\n",
            "DenseNet-121\tAverage runtime of ONNX Quantized Model in TPU: 114.83463389999997\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "************************\tEfficientNet-B0\t********************************\n",
            "\n",
            "\n",
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "EfficientNet-B0\tAverage runtime of ONNX Model in GPU: 39.56867629997305\n",
            "EfficientNet-B0\tAverage runtime of ONNX Optimized Model in GPU: 40.10078390003855\n",
            "quantized model saved to:EfficientNet-B0_opt_quant.onnx\n",
            "EfficientNet-B0\tONNX full precision model size (MB): 20.164688110351562\n",
            "EfficientNet-B0\tONNX quantized model size (MB): 5.379528999328613\n",
            "EfficientNet-B0\tAverage runtime of ONNX Model in TPU: 26.76917539997703\n",
            "EfficientNet-B0\tAverage runtime of ONNX Quantized Model in TPU: 59.04552249996868\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "************************\tConvNeXt-T\t********************************\n",
            "\n",
            "\n",
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "ConvNeXt-T\tAverage runtime of ONNX Model in GPU: 163.29668280000078\n",
            "ConvNeXt-T\tAverage runtime of ONNX Optimized Model in GPU: 167.82447880004838\n",
            "quantized model saved to:ConvNeXt-T_opt_quant.onnx\n",
            "ConvNeXt-T\tONNX full precision model size (MB): 109.16751003265381\n",
            "ConvNeXt-T\tONNX quantized model size (MB): 27.682156562805176\n",
            "ConvNeXt-T\tAverage runtime of ONNX Model in TPU: 166.7791882000074\n",
            "ConvNeXt-T\tAverage runtime of ONNX Quantized Model in TPU: 221.07910109996283\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i, model in enumerate(selected_models):\n",
        "    dummy_input = torch.rand(1, 3, 224, 224).to(device)\n",
        "    perform_onnx_infer(model, model_names[i], dummy_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n3bW24Cqw9_E"
      },
      "outputs": [],
      "source": [
        "!python - m onnxoptimizer ResNet-34.onnx ResNet-34_opt.onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-Xn4jSjwxwlt"
      },
      "outputs": [],
      "source": [
        "!python - m onnxoptimizer DenseNet-121.onnx DenseNet-121_opt.onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gSEZocK6zhbf"
      },
      "outputs": [],
      "source": [
        "!python - m onnxoptimizer EfficientNet-B0.onnx EfficientNet-B0_opt.onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ORIiiD2Pzij6"
      },
      "outputs": [],
      "source": [
        "!python - m onnxoptimizer ConvNeXt-T.onnx ConvNeXt-T_opt.onnx\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
