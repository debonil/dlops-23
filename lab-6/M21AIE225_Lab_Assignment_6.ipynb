{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfjzeQM6gzgy"
      },
      "source": [
        "# DL/DLOps (2023) Lab Assignment 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPl-JYIKnlyo",
        "outputId": "1dfe3a0c-bdca-4110-894c-4d730d231fee"
      },
      "outputs": [],
      "source": [
        "!pip install torch-tb-profiler\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install onnxoptimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0RmUBWWhQvT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "import torch.profiler\n",
        "import torch.utils.data\n",
        "import torchvision.datasets\n",
        "import torchvision.models\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1CVoLfll4Bc"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnqwBmOBF3qu",
        "outputId": "01195d77-5219-4c09-9dc4-087819ec508a"
      },
      "outputs": [],
      "source": [
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UD-5z56hEuZ"
      },
      "source": [
        "**Q1.**\n",
        "\n",
        "a. Load and preprocessing CIFAR100 dataset using standard augmentation and\n",
        "normalization techniques [10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jarv-MyJgnZh",
        "outputId": "4cc933bc-cf0a-4da5-e30f-bd2497619e93"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose(\n",
        "    [T.Resize(224),\n",
        "     T.ToTensor(),\n",
        "     T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrakdhFHhspA"
      },
      "source": [
        "b. Train the following models for 50 epoch and at the same time profile the model using Tensorboard during the training step [5*4 = 20 Marks]\n",
        "\n",
        "○ ResNet-34\n",
        "\n",
        "○ DenseNet-121\n",
        "\n",
        "○ EfficientNet-B0\n",
        "\n",
        "○ ConvNeXt-T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzmHfTwBlU6Q",
        "outputId": "f29c398c-6e22-475b-9a81-dc8489cb1ce3"
      },
      "outputs": [],
      "source": [
        "selected_models = [\n",
        "    torchvision.models.resnet34(pretrained=True).to(device),\n",
        "    torchvision.models.densenet121(pretrained=True).to(device),\n",
        "    torchvision.models.efficientnet_b0(pretrained=True).to(device),\n",
        "    torchvision.models.convnext_tiny(pretrained=True).to(device),\n",
        "]\n",
        "\n",
        "model_names = [\n",
        "    'ResNet-34', 'DenseNet-121', 'EfficientNet-B0', 'ConvNeXt-T'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhdEfhbuivCJ"
      },
      "outputs": [],
      "source": [
        "def train(data):\n",
        "    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC5HaYKNmHK2"
      },
      "outputs": [],
      "source": [
        "!rm - rf ./logs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rQa8UE-hr94"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i, model in enumerate(selected_models):\n",
        "    epoch = 50\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    criterion = torch.nn.CrossEntropyLoss().cuda(device)\n",
        "    model.train()\n",
        "    with torch.profiler.profile(\n",
        "            schedule=torch.profiler.schedule(\n",
        "                wait=1, warmup=1, active=3, repeat=2),\n",
        "            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
        "                './logs/'+model_names[i]),\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=True\n",
        "    ) as prof:\n",
        "        for e in range(epoch):\n",
        "            for step, batch_data in enumerate(train_loader):\n",
        "                # if step >= (1 + 1 + 3) * 2:\n",
        "                #    break\n",
        "                train(batch_data)\n",
        "                prof.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euj9koPlU56N"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUeaLwo4DdUG"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVlFHgnldmzE"
      },
      "outputs": [],
      "source": [
        "#!kill 1044\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard - -logdir = ./logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HQjDyMdoY7Z"
      },
      "source": [
        "**Q1.c.** Then perform the following model inferencing techniques on the above listed models\n",
        "[10*2 = 20 Marks]\n",
        "\n",
        "○ Torchscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjE6CQwQTDlj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from time import perf_counter\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "def timer(f, *args):\n",
        "    start = perf_counter()\n",
        "    f(*args)\n",
        "    return (1000 * (perf_counter() - start))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2gZ7Kzda3Ik"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def get_size(file):\n",
        "    return os.path.getsize(file)/(1024*1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkX3HzfPoXnR",
        "outputId": "a4ffcaf9-8165-40fb-d166-6a90a397791c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "torch_script_results = []\n",
        "for i, model in enumerate(selected_models):\n",
        "    mod_name = model_names[i]\n",
        "\n",
        "    dummy_input = torch.rand(1, 3, 224, 224).to(device)\n",
        "\n",
        "    # *** This is the TorchScript export\n",
        "    model_scripted = torch.jit.script(model)\n",
        "    frozen_mod = torch.jit.optimize_for_inference(model_scripted)\n",
        "    model.eval()\n",
        "    # Get the unscripted model's prediction...\n",
        "    unscripted_output = model(dummy_input)\n",
        "    # ...and do the same for the scripted version\n",
        "    scripted_output = model_scripted(dummy_input)\n",
        "    # ...and do the same for the scripted version\n",
        "    scripted_frozen_mod_output = frozen_mod(dummy_input)\n",
        "\n",
        "    unscripted_top5 = F.softmax(unscripted_output, dim=1).topk(\n",
        "        5).indices.cpu().numpy().squeeze()\n",
        "    scripted_top5 = F.softmax(scripted_output, dim=1).topk(\n",
        "        5).indices.cpu().numpy().squeeze()\n",
        "    frozen_scripted_top5 = F.softmax(scripted_frozen_mod_output, dim=1).topk(\n",
        "        5).indices.cpu().numpy().squeeze()\n",
        "    print()\n",
        "    print(f'{mod_name} Python model top 5 results:\\t  {unscripted_top5}')\n",
        "    print(f'{mod_name} TorchScript model top 5 results:\\t  {scripted_top5}')\n",
        "    print(f'{mod_name} TorchScript Frozen model top 5 results:\\t  {frozen_scripted_top5}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'models/{mod_name}_pytorch.pt')\n",
        "    model_scripted.save(f'models/{mod_name}_scripted.pt')\n",
        "    frozen_mod.save(f'models/{mod_name}_scripted_frzn.pt')\n",
        "\n",
        "    model_size = get_size(f'models/{mod_name}_pytorch.pt')\n",
        "    model_scripted_size = get_size(f'models/{mod_name}_scripted.pt')\n",
        "    frozen_mod_size = get_size(f'models/{mod_name}_scripted_frzn.pt')\n",
        "\n",
        "    avgRuntimePytorch = np.mean([timer(model, dummy_input) for _ in range(10)])\n",
        "    avgRuntimeTorchScript = np.mean(\n",
        "        [timer(model_scripted, dummy_input) for _ in range(10)])\n",
        "    avgRuntimeTorchScriptFrozen = np.mean(\n",
        "        [timer(frozen_mod, dummy_input) for _ in range(10)])\n",
        "    print()\n",
        "    print(f\"{mod_name} Average runtime of Pytorch Model in {device}: \\t\" +\n",
        "          str(avgRuntimePytorch))\n",
        "    print(f\"{mod_name} Average runtime of TorchScript Model in {device} : \\t\" +\n",
        "          str(avgRuntimeTorchScript))\n",
        "    print(f\"{mod_name} Average runtime of TorchScript Frozen Model in {device} : \\t\" +\n",
        "          str(avgRuntimeTorchScriptFrozen))\n",
        "    print()\n",
        "    print(f\"{mod_name} Size of Pytorch Model in {device}: \\t\" + str(model_size))\n",
        "    print(f\"{mod_name} Size of TorchScript Model in {device} : \\t\" +\n",
        "          str(model_scripted_size))\n",
        "    print(f\"{mod_name} Size of TorchScript Frozen Model in {device} : \\t\" +\n",
        "          str(frozen_mod_size))\n",
        "\n",
        "    torch_script_results.append({\n",
        "        \"Model\": mod_name,\n",
        "        \"model_size\": model_size,\n",
        "        \"model_scripted_size\": model_scripted_size,\n",
        "        \"frozen_mod_size\": frozen_mod_size,\n",
        "        \"avgRuntimePytorch\": avgRuntimePytorch,\n",
        "        \"avgRuntimeTorchScript\": avgRuntimeTorchScript,\n",
        "        \"avgRuntimeTorchScriptFrozen\": avgRuntimeTorchScriptFrozen,\n",
        "        \"unscripted_top5\": unscripted_top5,\n",
        "        \"scripted_top5\": scripted_top5,\n",
        "        \"frozen_scripted_top5\": frozen_scripted_top5,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwpOmyChTPUK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(torch_script_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "AOzM8EsGZcBW",
        "outputId": "588d3c46-61c4-4e89-c1fc-5f4f665fc020"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EkhhJ02sfUr"
      },
      "source": [
        "**Q1.c.** Then perform the following model inferencing techniques on the above listed models\n",
        "[10*2 = 20 Marks]\n",
        "\n",
        "○ ONNX "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch-tb-profiler\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install onnxoptimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from onnx import __version__, IR_VERSION\n",
        "from onnx.defs import onnx_opset_version\n",
        "print(\n",
        "    f\"onnx.__version__={__version__!r}, opset={onnx_opset_version()}, IR_VERSION={IR_VERSION}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from time import perf_counter\n",
        "\n",
        "import torch.onnx\n",
        "import onnx\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import onnxruntime\n",
        "import os\n",
        "\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "\n",
        "def time_ort_model_evaluation(model_path):\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session = onnxruntime.InferenceSession(model_path, sess_options)\n",
        "\n",
        "    time_per_inference = []\n",
        "    for _ in range(10):\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        # compute ONNX Runtime output prediction\n",
        "        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
        "        start = perf_counter()\n",
        "        session.run(None, ort_inputs)\n",
        "        time_per_inference.append((1000 * (perf_counter() - start)))\n",
        "\n",
        "    return np.mean(time_per_inference)\n",
        "\n",
        "\n",
        "def time_ort_model_evaluation(model_path):\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session = onnxruntime.InferenceSession(model_path, sess_options)\n",
        "\n",
        "    time_per_inference = []\n",
        "    for _ in range(10):\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        # compute ONNX Runtime output prediction\n",
        "        ort_inputs = {session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
        "        start = perf_counter()\n",
        "        session.run(None, ort_inputs)\n",
        "        time_per_inference.append((1000 * (perf_counter() - start)))\n",
        "\n",
        "    return np.mean(time_per_inference)\n",
        "\n",
        "\n",
        "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
        "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "    import onnx\n",
        "    onnx_opt_model = onnx.load(onnx_model_path)\n",
        "    quantize_dynamic(onnx_model_path,\n",
        "                     quantized_model_path,\n",
        "                     weight_type=QuantType.QUInt8)  # QInt8\n",
        "\n",
        "    print(f\"quantized model saved to:{quantized_model_path}\")\n",
        "\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "\n",
        "def perform_onnx_infer(model, model_name, dummy_input):\n",
        "\n",
        "    input_names = [\"actual_input\"]\n",
        "    output_names = [\"output\"]\n",
        "\n",
        "    print(\n",
        "        f'\\n\\n************************\\t{model_name}\\t********************************\\n\\n')\n",
        "\n",
        "    model_onnx = model_name + \".onnx\"\n",
        "    model_opt_onnx = model_name + \"_opt.onnx\"\n",
        "    model_opt_quant_onnx = model_name + \"_opt_quant.onnx\"\n",
        "\n",
        "    torch.onnx.export(model, dummy_input, model_onnx, verbose=False,\n",
        "                      input_names=input_names, output_names=output_names, export_params=True,)\n",
        "\n",
        "    ort_session = onnxruntime.InferenceSession(model_onnx)\n",
        "\n",
        "    # compute ONNX Runtime output prediction\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
        "    ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "    # compare ONNX Runtime and PyTorch results\n",
        "    torch_out = model(dummy_input)  # torch.randn(1, 3, 224, 224)\n",
        "    # np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "    os.system(f'!python -m onnxoptimizer {model_onnx} {model_opt_onnx}')\n",
        "\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Model in GPU: ' +\n",
        "          str(time_ort_model_evaluation(model_onnx)))\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Optimized Model in GPU: ' +\n",
        "          str(time_ort_model_evaluation(model_opt_onnx)))\n",
        "\n",
        "    quantize_onnx_model(model_opt_onnx, model_opt_quant_onnx)\n",
        "\n",
        "    print(f'{model_name}\\tONNX full precision model size (MB):',\n",
        "          os.path.getsize(model_opt_onnx)/(1024*1024))\n",
        "    print(f'{model_name}\\tONNX quantized model size (MB):', os.path.getsize(\n",
        "        model_opt_quant_onnx)/(1024*1024))\n",
        "\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Model in TPU: ' +\n",
        "          str(time_ort_model_evaluation(model_onnx)))\n",
        "    print(f'{model_name}\\tAverage runtime of ONNX Quantized Model in TPU: ' +\n",
        "          str(time_ort_model_evaluation(model_opt_quant_onnx)))\n",
        "    print()\n",
        "    print('-'*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, model in enumerate(selected_models):\n",
        "    dummy_input = torch.rand(1, 3, 224, 224).to(device)\n",
        "    perform_onnx_infer(model, model_names[i], dummy_input)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
