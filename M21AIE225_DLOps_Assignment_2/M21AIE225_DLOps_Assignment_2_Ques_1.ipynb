{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLOps Assignment 2: RNN, LSTM and Docker [100 Marks]\n",
    "\n",
    "### Submitted By Debonil Ghosh [M21AIE225]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1. You have been provided a DATASET, which contains pairs of the words (x,y) i.e. akhbaar अख़बार in which the first word is a Latin word( words we usually type while chatting with friends in WhatsApp) and the second word is its corresponding word in native script. Your main goal is to train a seq2seq model which takes as input the romanized string and produces the corresponding word in native script.\n",
    "For Example, Jabki yah Jainon se km hai. ⇒ जबकि यह जनै ों सेकम है। [75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word  romanization  no_of_attestation\n",
      "1508     टकराने      takraane                  1\n",
      "2995       मधेश       madhesh                  3\n",
      "3360     रामायण       ramayan                  2\n",
      "1818         दन           dan                  3\n",
      "3397  रिलेशनशिप  relationship                  3\n",
      "1755   तीरंदाजी     tirandaji                  1\n",
      "1816     दगाबाज       dagabaj                  1\n",
      "2358    पेपरवेट   paperweight                  2\n",
      "2281   पारदर्शी     pardarshi                  2\n",
      "3496     लटकाया       latkaya                  2\n",
      "338        इट्स           its                  3\n",
      "1292      जनसंघ     janasangh                  1\n",
      "1748   तिरस्कृत     tiraskrit                  1\n",
      "2198     पधारना     padharana                  2\n",
      "2188      पतीला       patilaa                  1\n",
      "742       कितनी         kitni                  2\n",
      "2718    बायोटेक       biotech                  3\n",
      "235        आचमन       aachman                  3\n",
      "3825   श्रद्धेय    shraddheya                  1\n",
      "3024   महागणपति   mahaganpati                  2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_dev = pd.read_csv('../downloads/Dakshina Dataset/hi/lexicons/hi.translit.sampled.dev.tsv',\n",
    "                       sep='\\t', names=['word', 'romanization', 'no_of_attestation'])\n",
    "data_test = pd.read_csv('../downloads/Dakshina Dataset/hi/lexicons/hi.translit.sampled.test.tsv',\n",
    "                        sep='\\t', names=['word', 'romanization', 'no_of_attestation'])\n",
    "data_train = pd.read_csv('../downloads/Dakshina Dataset/hi/lexicons/hi.translit.sampled.train.tsv',\n",
    "                         sep='\\t', names=['word', 'romanization', 'no_of_attestation'])\n",
    "\n",
    "print(data_dev.sample(20))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dakshina Dataset and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class LoadDakshinaDataset():\n",
    "\n",
    "    def __init__(self, DATAPATH, source_lang='en', target_lang=\"bn\"):\n",
    "\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "\n",
    "        self.trainpath = os.path.join(\n",
    "            DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.train.tsv\")\n",
    "        self.valpath = os.path.join(\n",
    "            DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.dev.tsv\")\n",
    "        self.testpath = os.path.join(\n",
    "            DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.test.tsv\")\n",
    "        self.train = pd.read_csv(\n",
    "            self.trainpath,\n",
    "            sep=\"\\t\",\n",
    "            names=[\"tgt\", \"src\", \"count\"],\n",
    "        )\n",
    "        self.val = pd.read_csv(\n",
    "            self.valpath,\n",
    "            sep=\"\\t\",\n",
    "            names=[\"tgt\", \"src\", \"count\"],\n",
    "        )\n",
    "        self.test = pd.read_csv(\n",
    "            self.testpath,\n",
    "            sep=\"\\t\",\n",
    "            names=[\"tgt\", \"src\", \"count\"],\n",
    "        )\n",
    "\n",
    "        # create train data\n",
    "        self.train_data = self.preprocess(\n",
    "            self.train[\"src\"].to_list(), self.train[\"tgt\"].to_list())\n",
    "        # print(self.train_data.shape)\n",
    "        # print(self.train_data[0])\n",
    "        (\n",
    "            self.train_encoder_input,\n",
    "            self.train_decoder_input,\n",
    "            self.train_decoder_target,\n",
    "            self.source_vocab,\n",
    "            self.target_vocab,\n",
    "        ) = self.train_data\n",
    "        self.source_char2int, self.source_int2char = self.source_vocab\n",
    "        self.target_char2int, self.target_int2char = self.target_vocab\n",
    "\n",
    "        self.trainloader = torch.utils.data.DataLoader(TensorDataset(torch.Tensor(\n",
    "            self.train_encoder_input, device=device), torch.Tensor(self.train_decoder_target, device=device)), batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "        # create val data (only encode function suffices as the dictionary lookup should be kep the same.\n",
    "        self.val_data = self.encode(\n",
    "            self.val[\"src\"].to_list(),\n",
    "            self.val[\"tgt\"].to_list(),\n",
    "            list(self.source_char2int.keys()),\n",
    "            list(self.target_char2int.keys()),\n",
    "            source_char2int=self.source_char2int,\n",
    "            target_char2int=self.target_char2int,\n",
    "        )\n",
    "        self.val_encoder_input, self.val_decoder_input, self.val_decoder_target = self.val_data\n",
    "        self.source_char2int, self.source_int2char = self.source_vocab\n",
    "        self.target_char2int, self.target_int2char = self.target_vocab\n",
    "\n",
    "        self.valloader = torch.utils.data.DataLoader(TensorDataset(torch.Tensor(\n",
    "            self.val_encoder_input, device=device), torch.Tensor(self.val_decoder_target, device=device)), batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "        # create test data\n",
    "        self.test_data = self.encode(\n",
    "            self.test[\"src\"].to_list(),\n",
    "            self.test[\"tgt\"].to_list(),\n",
    "            list(self.source_char2int.keys()),\n",
    "            list(self.target_char2int.keys()),\n",
    "            source_char2int=self.source_char2int,\n",
    "            target_char2int=self.target_char2int,\n",
    "        )\n",
    "        self.test_encoder_input, self.test_decoder_input, self.test_decoder_target = self.test_data\n",
    "        self.source_char2int, self.source_int2char = self.source_vocab\n",
    "        self.target_char2int, self.target_int2char = self.target_vocab\n",
    "\n",
    "        self.testloader = torch.utils.data.DataLoader(TensorDataset(torch.Tensor(\n",
    "            self.test_encoder_input, device=device), torch.Tensor(self.test_decoder_target, device=device)), batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "    def dictionary_lookup(self, vocab):\n",
    "        char2int = dict([(char, i) for i, char in enumerate(vocab)])\n",
    "        int2char = dict((i, char) for char, i in char2int.items())\n",
    "        return char2int, int2char\n",
    "\n",
    "    def encode(self, source, target, source_chars, target_chars, source_char2int=None, target_char2int=None):\n",
    "        #print(f\"encode ==> source:{source[500:510]}\")\n",
    "        #print(f\"encode ==> target:{target[500:510]}\")\n",
    "        num_encoder_tokens = len(source_chars)\n",
    "        num_decoder_tokens = len(target_chars)\n",
    "        max_source_length = max([len(txt) for txt in source])\n",
    "        max_target_length = max([len(txt) for txt in target])\n",
    "\n",
    "        source_vocab, target_vocab = None, None\n",
    "        if source_char2int == None and target_char2int == None:\n",
    "            print(\n",
    "                \"Generating the dictionary lookups for character to integer mapping and back\")\n",
    "            source_char2int, source_int2char = self.dictionary_lookup(\n",
    "                source_chars)\n",
    "            target_char2int, target_int2char = self.dictionary_lookup(\n",
    "                target_chars)\n",
    "\n",
    "            source_vocab = (source_char2int, source_int2char)\n",
    "            target_vocab = (target_char2int, target_int2char)\n",
    "\n",
    "        encoder_input_data = np.zeros(\n",
    "            (len(source), max_source_length+1, num_encoder_tokens), dtype=\"int32\"\n",
    "        )\n",
    "        decoder_input_data = np.zeros(\n",
    "            (len(source), max_target_length, num_decoder_tokens), dtype=\"int32\"\n",
    "        )\n",
    "        decoder_target_data = np.zeros(\n",
    "            (len(source), max_target_length, num_decoder_tokens), dtype=\"int32\"\n",
    "        )\n",
    "\n",
    "        for i, (input_text, target_text) in enumerate(zip(source, target)):\n",
    "            for t, char in enumerate(input_text):\n",
    "                encoder_input_data[i, t, source_char2int[char]] = 1.0\n",
    "            encoder_input_data[i, t + 1:, source_char2int[\" \"]] = 1.0\n",
    "            for t, char in enumerate(target_text):\n",
    "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                decoder_input_data[i, t, target_char2int[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    # and will not include the start character.\n",
    "                    decoder_target_data[i, t - 1, target_char2int[char]] = 1.0\n",
    "            decoder_input_data[i, t + 1:, target_char2int[\" \"]] = 1.0\n",
    "            decoder_target_data[i, t:, target_char2int[\" \"]] = 1.0\n",
    "\n",
    "        if source_vocab != None and target_vocab != None:\n",
    "            return (\n",
    "                encoder_input_data,\n",
    "                decoder_input_data,\n",
    "                decoder_target_data,\n",
    "                source_vocab,\n",
    "                target_vocab,\n",
    "            )\n",
    "        else:\n",
    "            return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "    def preprocess(self, source, target):\n",
    "        source_chars = set()\n",
    "        target_chars = set()\n",
    "\n",
    "        source = [str(x) for x in source]\n",
    "        target = [str(x) for x in target]\n",
    "\n",
    "        source_words = []\n",
    "        target_words = []\n",
    "        for src, tgt in zip(source, target):\n",
    "            tgt = \"\\t\" + tgt + \"\\n\"\n",
    "            source_words.append(src)\n",
    "            target_words.append(tgt)\n",
    "            for char in src:\n",
    "                if char not in source_chars:\n",
    "                    source_chars.add(char)\n",
    "            for char in tgt:\n",
    "                if char not in target_chars:\n",
    "                    target_chars.add(char)\n",
    "\n",
    "        source_chars = sorted(list(source_chars))\n",
    "        target_chars = sorted(list(target_chars))\n",
    "\n",
    "        # The space needs to be appended so that the encode function doesn't throw errors\n",
    "        source_chars.append(\" \")\n",
    "        target_chars.append(\" \")\n",
    "\n",
    "        num_encoder_tokens = len(source_chars)\n",
    "        num_decoder_tokens = len(target_chars)\n",
    "        max_source_length = max([len(txt) for txt in source_words])\n",
    "        max_target_length = max([len(txt) for txt in target_words])\n",
    "\n",
    "        print(\"Number of samples:\", len(source))\n",
    "        print(\"Source Vocab length:\", num_encoder_tokens)\n",
    "        print(\"Target Vocab length:\", num_decoder_tokens)\n",
    "        print(\"Max sequence length for inputs:\", max_source_length)\n",
    "        print(\"Max sequence length for outputs:\", max_target_length)\n",
    "\n",
    "        return self.encode(source_words, target_words, source_chars, target_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 44204\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 66\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 21\n",
      "Generating the dictionary lookups for character to integer mapping and back\n"
     ]
    }
   ],
   "source": [
    "dataset = LoadDakshinaDataset(\n",
    "    '../downloads/Dakshina Dataset', source_lang='en', target_lang='hi')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Build a seq2seq model which contains the following layers - [20]\n",
    "\n",
    "i) input layer for character embeddings\n",
    "\n",
    "ii) one encoder which sequentially encodes the input character sequence (Latin)\n",
    "\n",
    "iii) one decoder which takes the last state of the encoder as an input and produces one\n",
    "character output at a time (native).\n",
    "\n",
    "\n",
    "Please note that the dimension of input character embeddings, the hidden state of\n",
    "encoders and decoders, the cell(RNN and LSTM), and the number of layers in the encoder and\n",
    "decoder should be passed as an argument.\n",
    "(Note:- For Reference you may refer to this Blog, but the implementation must be in\n",
    "PyTorch only.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cell_type, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        if self.cell_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(\n",
    "                input_dim, emb_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_dim, emb_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src : [sen_len, batch_size]\n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        # embedded : [sen_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(src)\n",
    "        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n",
    "        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_direction, batch_size, hid_dim]\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cell_type, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        if self.cell_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(\n",
    "                emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(\n",
    "                emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "       # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "        # input = [batch_size]\n",
    "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "        # input : [1, ,batch_size]\n",
    "\n",
    "        # embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch_size, emb_dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(input, (hidden, cell))\n",
    "        # output = [seq_len, batch_size, hid_dim * n_dir]\n",
    "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
    "\n",
    "        # seq_len and n_dir will always be 1 in the decoder\n",
    "        prediction = output.squeeze(0)\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'hidden dimensions of encoder and decoder must be equal.'\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'n_layers of encoder and decoder must be equal.'\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [sen_len, batch_size]\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size,\n",
    "                              trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token.\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states.\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # replace predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if we are going to use teacher forcing or not.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # get the highest predicted token from our predictions.\n",
    "            top1 = output.argmax(1)\n",
    "            # update input : use ground_truth when teacher_force\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "class S2STranslation():\n",
    "\n",
    "    def __init__(self, modelConfigDict, srcChar2Int, tgtChar2Int):\n",
    "        print(modelConfigDict)\n",
    "        #self.native_vocabulary = modelConfigDict[\"native_vocabulary\"]\n",
    "        self.numEncoders = modelConfigDict[\"numEncoders\"]\n",
    "        self.cell_type = modelConfigDict[\"cell_type\"]\n",
    "        self.latentDim = modelConfigDict[\"latentDim\"]\n",
    "        self.dropout = modelConfigDict[\"dropout\"]\n",
    "        self.numDecoders = modelConfigDict[\"numDecoders\"]\n",
    "        self.hidden = modelConfigDict[\"hidden\"]\n",
    "\n",
    "        self.tgtChar2Int = tgtChar2Int\n",
    "        self.srcChar2Int = srcChar2Int\n",
    "\n",
    "        self.epochs = modelConfigDict[\"epochs\"]\n",
    "        self.batch_size = modelConfigDict[\"batch_size\"]\n",
    "        self.optimiser = modelConfigDict[\"optimiser\"]\n",
    "        self.optimiser_patience = modelConfigDict[\"optimiser_patience\"]\n",
    "\n",
    "    def build_configurable_model(self):\n",
    "\n",
    "        encoder = Encoder(self.cell_type, len(\n",
    "            self.srcChar2Int), self.latentDim, self.hidden, self.numEncoders, self.dropout)\n",
    "        decoder = Decoder(self.cell_type, len(\n",
    "            self.tgtChar2Int), self.latentDim, self.hidden, self.numDecoders, self.dropout)\n",
    "\n",
    "        self.model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "        return self.model\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        parameters = sum(p.numel()\n",
    "                         for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f'The model has {parameters:,} trainable parameters')\n",
    "        return parameters\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "        self.count_parameters()\n",
    "\n",
    "    def train_epoch(self, iterator, optimizer, criterion, clip):\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for local_batch, local_labels in iterator:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # trg = [sen_len, batch_size]\n",
    "            # output = [trg_len, batch_size, output_dim]\n",
    "            output = self.model(local_batch, local_labels)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            local_labels = local_labels[1:].view(-1)\n",
    "            # trg = [(trg_len-1) * batch_size]\n",
    "            # output = [(trg_len-1) * batch_size, output_dim]\n",
    "\n",
    "            loss = criterion(output, local_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    def train(self, train_iter, valid_iter):\n",
    "        CLIP = 1\n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss = self.train_epoch(\n",
    "                train_iter, optimizer, criterion, CLIP)\n",
    "            valid_loss = self.evaluate(valid_iter, criterion)\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(self.model.state_dict(), 'Seq2SeqModel.pt')\n",
    "            print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
    "            print(\n",
    "                f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "            print(\n",
    "                f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    # Plot the training history\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44204, 21, 27), (44204, 21, 66)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset.train_encoder_input.shape, dataset.train_decoder_input.shape]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "\n",
    "    modelInit = S2STranslation(\n",
    "        config, srcChar2Int=dataset.source_char2int, tgtChar2Int=dataset.target_char2int)\n",
    "\n",
    "    model = modelInit.build_configurable_model()\n",
    "\n",
    "    modelInit.summary()\n",
    "\n",
    "    train_hist = modelInit.train(dataset.trainloader, dataset.valloader)\n",
    "\n",
    "    plot_history(train_hist)\n",
    "\n",
    "    return model, train_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell_type': 'LSTM', 'latentDim': 256, 'hidden': 64, 'optimiser': 'adam', 'numEncoders': 1, 'numDecoders': 1, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'optimiser_patience': 5}\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn): LSTM(27, 256, dropout=0.1)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn): LSTM(256, 64, dropout=0.1)\n",
      "    (fc_out): Linear(in_features=64, out_features=66, bias=True)\n",
      "  )\n",
      ")\n",
      "The model has 378,562 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 256, got 66",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\MTechClasses\\DL-Ops\\dlops-23\\M21AIE225_DLOps_Assignment_2\\M21AIE225_DLOps_Assignment_2.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m config_default \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlatentDim\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moptimiser_patience\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model, _ \u001b[39m=\u001b[39m train(config_default)\n",
      "\u001b[1;32mg:\\MTechClasses\\DL-Ops\\dlops-23\\M21AIE225_DLOps_Assignment_2\\M21AIE225_DLOps_Assignment_2.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m modelInit\u001b[39m.\u001b[39mbuild_configurable_model()\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m modelInit\u001b[39m.\u001b[39msummary()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_hist \u001b[39m=\u001b[39m modelInit\u001b[39m.\u001b[39;49mtrain(dataset\u001b[39m.\u001b[39;49mtrainloader, dataset\u001b[39m.\u001b[39;49mvalloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plot_history(train_hist)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model, train_hist\n",
      "\u001b[1;32mg:\\MTechClasses\\DL-Ops\\dlops-23\\M21AIE225_DLOps_Assignment_2\\M21AIE225_DLOps_Assignment_2.ipynb Cell 13\u001b[0m in \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m         train_iter, optimizer, criterion, CLIP)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(valid_iter, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32mg:\\MTechClasses\\DL-Ops\\dlops-23\\M21AIE225_DLOps_Assignment_2\\M21AIE225_DLOps_Assignment_2.ipynb Cell 13\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# trg = [sen_len, batch_size]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# output = [trg_len, batch_size, output_dim]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(local_batch, local_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# transfrom our output : slice off the first column, and flatten the output into 2 dim.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mg:\\MTechClasses\\DL-Ops\\dlops-23\\M21AIE225_DLOps_Assignment_2\\M21AIE225_DLOps_Assignment_2.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m trg[\u001b[39m0\u001b[39m, :]\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, trg_len):\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39m# insert input token embedding, previous hidden and previous cell states\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39m# receive output tensor (predictions) and new hidden and cell states.\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     output, hidden, cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\u001b[39minput\u001b[39;49m, hidden, cell)\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     \u001b[39m# replace predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n",
      "File \u001b[1;32mc:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mg:\\MTechClasses\\DL-Ops\\dlops-23\\M21AIE225_DLOps_Assignment_2\\M21AIE225_DLOps_Assignment_2.ipynb Cell 13\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# input : [1, ,batch_size]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# embedded = self.dropout(self.embedding(input))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# embedded = [1, batch_size, emb_dim]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m output, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(\u001b[39minput\u001b[39;49m, (hidden, cell))\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# output = [seq_len, batch_size, hid_dim * n_dir]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# hidden = [n_layers * n_dir, batch_size, hid_dim]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# cell = [n_layers * n_dir, batch_size, hid_dim]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# seq_len and n_dir will always be 1 in the decoder\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MTechClasses/DL-Ops/dlops-23/M21AIE225_DLOps_Assignment_2/M21AIE225_DLOps_Assignment_2.ipynb#X15sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m prediction \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\debon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 66"
     ]
    }
   ],
   "source": [
    "config_default = {\n",
    "    \"cell_type\": \"LSTM\",\n",
    "    \"latentDim\": 256,\n",
    "    \"hidden\": 64,\n",
    "    \"optimiser\": 'adam',\n",
    "    \"numEncoders\": 1,\n",
    "    \"numDecoders\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    'optimiser_patience': 5\n",
    "}\n",
    "model, _ = train(config_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_encoder_input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_decoder_input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'om                  '"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([dataset.source_int2char[x]\n",
    "    for x in dataset.train_encoder_input[-1].argmax(axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(''.join([dataset.target_int2char[x]\n",
    "    for x in dataset.train_decoder_input[-1].argmax(axis=1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(TensorDataset(torch.tensor(dataset.train_encoder_input), torch.tensor(\n",
    "    dataset.train_decoder_target)), batch_size=32, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for local_batch, local_labels in trainloader:\n",
    "    print([local_batch.shape, local_labels.shape])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ae7141c631147330982ab03a122191846cf83f5ea8efac7ef9984176873ce7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
