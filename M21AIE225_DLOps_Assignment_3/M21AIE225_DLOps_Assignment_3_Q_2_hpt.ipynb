{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slp93mmeoq2U"
      },
      "source": [
        "## DL Ops Assignment 3\n",
        "### Question 2\n",
        "#### Submitted by - Debonil Ghosh [ M21AIE225 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxcJ9hKuMVLl",
        "outputId": "8db29894-267c-4c63-9bd8-eaf8afe76ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available device ==> cuda:0\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets\n",
        "import torchvision.models\n",
        "import torchvision.transforms as T\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Available device ==> {device}')\n",
        "\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "# %%\n",
        "!rm - rf ./logs\n",
        "# Train the model\n",
        "\n",
        "\n",
        "def model_training(model, criterion, optimizer, trainloader, testloader, num_epochs=10, model_name='model'):\n",
        "    start = time.time()\n",
        "    loss_list = []\n",
        "    model.train()\n",
        "    with torch.profiler.profile(\n",
        "            schedule=torch.profiler.schedule(\n",
        "                wait=1, warmup=1, active=3, repeat=2),\n",
        "            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
        "                './logs/'+model_name),\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=True\n",
        "    ) as prof:\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start = time.time()\n",
        "            train_loss = 0.0\n",
        "            val_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "            val_acc = 0.0\n",
        "            for images, labels in trainloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_acc += metrics.accuracy_score(labels.cpu().detach(\n",
        "                ).numpy(), outputs.cpu().detach().numpy().argmax(axis=1))\n",
        "                prof.step()\n",
        "            # Evaluate the model on the validation set\n",
        "            with torch.no_grad():\n",
        "                for images, labels in testloader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                    val_acc += metrics.accuracy_score(labels.cpu().detach(\n",
        "                    ).numpy(), outputs.cpu().detach().numpy().argmax(axis=1))\n",
        "            train_loss = train_loss/len(trainloader)\n",
        "            val_loss = val_loss/len(testloader)\n",
        "            train_acc = train_acc/len(trainloader)\n",
        "            val_acc = val_acc/len(testloader)\n",
        "            print(f'Epoch: {epoch+1} ({timeSince(epoch_start)}) \\tTraining Loss: {train_loss:.3f}, \\tTest Loss: {val_loss:.3f},  \\tTraining acc: {train_acc:.2f}, \\tTest acc: {val_acc:.2f}, ')\n",
        "            loss_list.append([train_loss, val_loss, train_acc, val_acc])\n",
        "            trial.report(val_acc, epoch)\n",
        "\n",
        "            # Handle pruning based on the intermediate value.\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "        print(\n",
        "            f'Training completed in {timeSince(start)} \\tTraining Loss: {loss_list[-1][0]:.3f}, \\tTest Loss: {loss_list[-1][1]:.3f},  \\tTraining acc: {loss_list[-1][2]:.2f}, \\tTest acc: {loss_list[-1][3]:.2f}, ')\n",
        "        return np.array(loss_list), time.time()-start, loss_list[-1][2], loss_list[-1][3]\n",
        "\n",
        "\n",
        "# %%\n",
        "#sns.set(rc={'axes.facecolor': 'lightblue', 'figure.facecolor': 'lightblue'})\n",
        "\n",
        "\n",
        "def confusionMatrixAndAccuracyReport(Y_test, Y_pred, classes, title=''):\n",
        "    cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
        "    overallAccuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "\n",
        "    classwiseAccuracy = cm.diagonal()/cm.sum(axis=1)\n",
        "\n",
        "    f1_score = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.title(\n",
        "        f'{title} : Accuracy : {overallAccuracy*100:3.2f}% | F1 Score : {f1_score*100:3.2f}% ', size=14)\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "    cm.index.name = 'True Label'\n",
        "    cm.columns.name = 'Predicted Label'\n",
        "    sns.heatmap(data=cm, annot=True, square=True,  cmap='Blues',\n",
        "                fmt='g', xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "    plt.show()\n",
        "    plt.savefig(\n",
        "        f'confusion_mat_{title}_{time.time()}.png', bbox_inches='tight')\n",
        "    print(f'Accuracy: {overallAccuracy*100:3.3f}%')\n",
        "    print(f'F1 Score: {f1_score*100:3.3f}%')\n",
        "    classwiseAccuracy_df = pd.DataFrame(\n",
        "        data=[classwiseAccuracy], columns=classes)\n",
        "    print(\n",
        "        f'\\nClasswise Accuracy Score: \\n{classwiseAccuracy_df.to_markdown(index=False)}')\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(cm.to_markdown())\n",
        "\n",
        "\n",
        "# %%\n",
        "def plot_training_graphs(loss_list, title=''):\n",
        "    fig = plt.figure(figsize=(20, 7))\n",
        "    plot = fig.add_subplot(1, 2, 1)\n",
        "    plot.set_title(f\"{title} : Training vs Validation loss\")\n",
        "    plot.plot(loss_list[:, 0], linestyle='--', label=\"Training Loss\")\n",
        "    plot.plot(loss_list[:, 1], linestyle='-', label=\"Validation Loss\")\n",
        "    plot.set_xlabel(\"Epoch\")\n",
        "    plot.set_ylabel(\"Loss\")\n",
        "    plot.legend()\n",
        "    plot = fig.add_subplot(1, 2, 2)\n",
        "    plot.set_title(f\"{title} : Training vs Validation Accuracy\")\n",
        "    plot.plot(loss_list[:, 2], linestyle='--', label=\"Training Accuracy\")\n",
        "    plot.plot(loss_list[:, 3], linestyle='-', label=\"Validation Accuracy\")\n",
        "    plot.set_xlabel(\"Epoch\")\n",
        "    plot.set_ylabel(\"Accuracy\")\n",
        "    plot.legend()\n",
        "    plt.show()\n",
        "    plt.savefig(\n",
        "        f'training_loss_{title}_{time.time()}.png', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld9CNFZWoq2b"
      },
      "source": [
        " Load and preprocessing CIFAR10 dataset using standard augmentation and normalization techniques [10 Marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15q0g8KKoq2d",
        "outputId": "6d22be3c-8a1d-4651-e62e-f9d78a0b8a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocessing CIFAR10 dataset using standard augmentation and\n",
        "# normalization techniques [10 Marks]\n",
        "# %%\n",
        "data_path = '../.data'\n",
        "transform = T.Compose(\n",
        "    [T.ToTensor()])\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=data_path, train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=32, shuffle=True)\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=data_path, train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw2zg8dNoq2d"
      },
      "source": [
        "## Train the following models for profiling them using during the training step [5 *2 = 10Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zLmKB-5oq2d"
      },
      "source": [
        "A. Conv -> Conv -> Maxpool (2,2) -> Conv -> Maxpool(2,2) -> Conv -> Maxpool(2,2)\n",
        "\n",
        "i. You can decide the parameters of convolution layers and activations on your own.\n",
        "\n",
        "ii. Make sure to keep 4 conv-layers and 3 max-pool layers in the order describes above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmsR7aZ4oq2e",
        "outputId": "4cc5d4df-7a64-4790-f98f-c0a23bd1daaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomImageClassifier(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU()\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU()\n",
            "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU()\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Dropout(p=0.2, inplace=False)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CustomImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomImageClassifier, self).__init__()\n",
        "        # batch_size = 32\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16,\n",
        "                      kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32,\n",
        "                      kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64,\n",
        "                      kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128,\n",
        "                      kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=128, out_channels=256,\n",
        "                      kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(in_features=256*4*4, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(in_features=1024, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=10),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, 256*4*4)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "custom_cnn_model = CustomImageClassifier().to(device)\n",
        "print(custom_cnn_model.train())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4EGVpuysjmHh"
      },
      "outputs": [],
      "source": [
        "def train_custom_cnn_model(trial):\n",
        "    model_name = 'custom_cnn_model'\n",
        "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-1, log=True)\n",
        "    optimizer = torch.optim.Adam(custom_cnn_model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    loss_list, t, train_a, test_a = model_training(\n",
        "        custom_cnn_model, criterion, optimizer, trainloader, testloader, num_epochs=10, model_name=model_name)\n",
        "    plot_training_graphs(loss_list, title=model_name)\n",
        "    custom_cnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_labels = []\n",
        "        test_output = []\n",
        "        for batch in testloader:\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = custom_cnn_model(x)\n",
        "            test_labels += y.cpu()\n",
        "            test_output += torch.argmax(y_hat, dim=1).cpu()\n",
        "\n",
        "        test_labels = np.array(test_labels)\n",
        "        test_output = np.array(test_output)\n",
        "        print(f'\\nModel Evaluation Summary:')\n",
        "        confusionMatrixAndAccuracyReport(\n",
        "            test_labels, test_output, test_set.classes, title=model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfhOER2_oq2e"
      },
      "source": [
        "○ VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2BhDedWb-hp",
        "outputId": "4192f1a6-ecad-48a4-cf6f-46b6ac56e981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models\n",
        "vgg16_model = torchvision.models.vgg16(\n",
        "    weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1).to(device)\n",
        "print(vgg16_model.train())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "64ZhGsFroq2i"
      },
      "outputs": [],
      "source": [
        "def train_vgg16_model():\n",
        "    model_name = 'vgg16_model'\n",
        "    optimizer = torch.optim.Adam(vgg16_model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    loss_list, t, train_a, test_a = model_training(\n",
        "        vgg16_model, criterion, optimizer, trainloader, testloader, num_epochs=10, model_name=model_name)\n",
        "    plot_training_graphs(loss_list, title=model_name)\n",
        "    vgg16_model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_labels = []\n",
        "        test_output = []\n",
        "        for batch in testloader:\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = vgg16_model(x)\n",
        "            test_labels += y.cpu()\n",
        "            test_output += torch.argmax(y_hat, dim=1).cpu()\n",
        "\n",
        "        test_labels = np.array(test_labels)\n",
        "        test_output = np.array(test_output)\n",
        "        print(f'\\nModel Evaluation Summary:')\n",
        "        confusionMatrixAndAccuracyReport(\n",
        "            test_labels, test_output, test_set.classes, title=model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "     ------------------------------------ 365.7/365.7 kB 287.8 kB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\debon\\appdata\\roaming\\python\\python310\\site-packages (from optuna) (21.3)\n",
            "Collecting sqlalchemy>=1.3.0\n",
            "  Downloading SQLAlchemy-2.0.9-cp310-cp310-win_amd64.whl (2.0 MB)\n",
            "     ---------------------------------------- 2.0/2.0 MB 1.7 MB/s eta 0:00:00\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n",
            "     -------------------------------------- 212.3/212.3 kB 1.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in c:\\users\\debon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optuna) (1.23.4)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\debon\\appdata\\roaming\\python\\python310\\site-packages (from optuna) (6.0)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\debon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4 in c:\\users\\debon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "     ---------------------------------------- 78.7/78.7 kB 4.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\debon\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Collecting greenlet!=0.4.17\n",
            "  Downloading greenlet-2.0.2-cp310-cp310-win_amd64.whl (192 kB)\n",
            "     -------------------------------------- 192.2/192.2 kB 1.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\users\\debon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\debon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
            "Installing collected packages: Mako, greenlet, colorlog, cmaes, sqlalchemy, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.3 cmaes-0.9.1 colorlog-6.7.0 greenlet-2.0.2 optuna-3.1.1 sqlalchemy-2.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(train_custom_cnn_model, n_trials=100, timeout=60000)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(\n",
        "    deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
