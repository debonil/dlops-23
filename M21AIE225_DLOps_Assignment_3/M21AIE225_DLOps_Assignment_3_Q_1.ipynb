{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models\n",
    "import torchvision.datasets\n",
    "import torch.utils.data\n",
    "import torch.optim\n",
    "import torch.nn\n",
    "import torch\n",
    "print('Starting ...')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Available device ==> {device}')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def model_training(model, criterion, optimizer, trainloader, testloader, num_epochs=10, model_name='model'):\n",
    "    start = time.time()\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += metrics.accuracy_score(labels.cpu().detach(\n",
    "            ).numpy(), outputs.cpu().detach().numpy().argmax(axis=1))\n",
    "        # Evaluate the model on the validation set\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += metrics.accuracy_score(labels.cpu().detach(\n",
    "                ).numpy(), outputs.cpu().detach().numpy().argmax(axis=1))\n",
    "        train_loss = train_loss/len(trainloader)\n",
    "        val_loss = val_loss/len(testloader)\n",
    "        train_acc = train_acc/len(trainloader)\n",
    "        val_acc = val_acc/len(testloader)\n",
    "        print(f'Epoch: {epoch+1} ({timeSince(start)}) \\tTraining Loss: {train_loss:.3f}, \\tTest Loss: {val_loss:.3f},  \\tTraining acc: {train_acc:.2f}, \\tTest acc: {val_acc:.2f}, ', flush=True)\n",
    "        loss_list.append([train_loss, val_loss, train_acc, val_acc])\n",
    "\n",
    "    print(\n",
    "        f'Training completed in {timeSince(start)} \\tTraining Loss: {loss_list[-1][0]:.3f}, \\tTest Loss: {loss_list[-1][1]:.3f},  \\tTraining acc: {loss_list[-1][2]:.2f}, \\tTest acc: {loss_list[-1][3]:.2f}, ')\n",
    "    return np.array(loss_list), time.time()-start, loss_list[-1][2], loss_list[-1][3]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(rc={'axes.facecolor': 'lightblue', 'figure.facecolor': 'lightblue'})\n",
    "\n",
    "\n",
    "def confusionMatrixAndAccuracyReport(Y_test, Y_pred_probs, classes):\n",
    "    Y_pred = Y_pred_probs.argmax(axis=1)\n",
    "    cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "    overallAccuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "    classwiseAccuracy = cm.diagonal()/cm.sum(axis=1)\n",
    "\n",
    "    top_5_accuracy = metrics.top_k_accuracy_score(\n",
    "        Y_test, Y_pred_probs, k=5, labels=np.arange(10))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\n",
    "        f'Top 1 Accuracy : {overallAccuracy*100:3.2f}% | Top 5 Accuracy : {top_5_accuracy*100:3.2f}% ', size=14)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    sns.heatmap(data=cm, annot=True, square=True,  cmap='Blues',\n",
    "                fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(f'confusion_mat_{time.time()}.png', bbox_inches='tight')\n",
    "    print(f'Top 1 Accuracy: {overallAccuracy*100:3.3f}%')\n",
    "    print(f'Top 5 Accuracy: {top_5_accuracy*100}%')\n",
    "    print(f'Classwise Accuracy Score: \\n{classwiseAccuracy}')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_graphs(loss_list):\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    plot = fig.add_subplot(1, 2, 1)\n",
    "    plot.set_title(\"Training vs Validation loss\")\n",
    "    plot.plot(loss_list[:, 0], linestyle='--', label=\"Training Loss\")\n",
    "    plot.plot(loss_list[:, 1], linestyle='-', label=\"Validation Loss\")\n",
    "    plot.set_xlabel(\"Epoch\")\n",
    "    plot.set_ylabel(\"Loss\")\n",
    "    plot.legend()\n",
    "    plot = fig.add_subplot(1, 2, 2)\n",
    "    plot.set_title(\"Training vs Validation Accuracy\")\n",
    "    plot.plot(loss_list[:, 2], linestyle='--', label=\"Training Accuracy\")\n",
    "    plot.plot(loss_list[:, 3], linestyle='-', label=\"Validation Accuracy\")\n",
    "    plot.set_xlabel(\"Epoch\")\n",
    "    plot.set_ylabel(\"Accuracy\")\n",
    "    plot.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(f'training_loss_{time.time()}.png', bbox_inches='tight')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_patches_from_images(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i + 1) * patch_size,\n",
    "                              j * patch_size: (j + 1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, d, n_heads=2, activation_fn=torch.nn.Softmax):\n",
    "        super(MultiHeadSelfAttentionBlock, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.activation_fn = activation_fn(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.activation_fn(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "\n",
    "class VisionTransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, mlp_ratio=4, mlp_activation=torch.nn.GELU, msa_activation=torch.nn.Softmax):\n",
    "        super(VisionTransformerBlock, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.mhsa = MultiHeadSelfAttentionBlock(\n",
    "            hidden_dim, n_heads, msa_activation)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, mlp_ratio * hidden_dim),\n",
    "            mlp_activation(),\n",
    "            torch.nn.Linear(mlp_ratio * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n",
    "    \n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.pe = torch.nn.Parameter(torch.zeros(max_len, d_model))\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class VisionTransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_dim=8, n_heads=2, out_d=10, activation_fn=torch.nn.ReLU, learn_pos_emb = False):\n",
    "        # Super constructor\n",
    "        super(VisionTransformerClassifier, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.chw = chw  # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_activation = torch.nn.GELU if activation_fn == None else activation_fn\n",
    "        self.msa_activation = torch.nn.Softmax if activation_fn == None else activation_fn\n",
    "        self.learn_pos_emb = learn_pos_emb\n",
    "\n",
    "        # Input and patches sizes\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_dim)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = torch.nn.Parameter(torch.rand(1, self.hidden_dim))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        if learn_pos_emb:\n",
    "            self.positional_embeddings = PositionalEmbedding(hidden_dim, n_patches ** 2 + 1)\n",
    "        else:\n",
    "            self.register_buffer('positional_embeddings', get_positional_embeddings(\n",
    "                n_patches ** 2 + 1, hidden_dim), persistent=False)\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "            [VisionTransformerBlock(hidden_dim, n_heads, mlp_activation=activation_fn) for _ in range(n_blocks)])\n",
    "\n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, out_d),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = split_patches_from_images(images, self.n_patches).to(\n",
    "            self.positional_embeddings.device)\n",
    "\n",
    "        # Running linear layer tokenization\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        # Map to output dimension, output category distribution\n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(\n",
    "                i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def view_samples(testloader, classes):\n",
    "\n",
    "    test_images, labels = next(iter(testloader))\n",
    "    print(test_images.shape)\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    nrows = 3\n",
    "    ncols = 10\n",
    "    b = np.random.randint(0, test_images.shape[0]-nrows*ncols)\n",
    "    for i in range(nrows*ncols):\n",
    "        plot = fig.add_subplot(nrows, ncols, i+1)\n",
    "        plot.set_title(classes[labels[i+b].cpu().numpy()])\n",
    "        plot.imshow(np.transpose(test_images[i+b], (1, 2, 0)).cpu())\n",
    "    plt.show()\n",
    "    plt.savefig(f'view_samples_{time.time()}.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "def filter_dataset(dataset_full):\n",
    "    # Selecting even classes 0,2,4,6,8 as roll number is odd (M21AIE225)\n",
    "    targets = np.array(dataset_full.targets)\n",
    "    idx = (targets == 0) | (targets == 2) | (\n",
    "        targets == 4) | (targets == 6) | (targets == 8)\n",
    "    dataset_full.targets = np.rint(targets[idx]/2).astype(int)\n",
    "    dataset_full.data = dataset_full.data[idx]\n",
    "    dataset_full.classes = [dataset_full.classes[c] for c in [0, 2, 4, 6, 8]]\n",
    "    return dataset_full\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Loading ...')\n",
    "transform = T.Compose(\n",
    "    [T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_set = filter_dataset(train_set)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=256, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "test_set = filter_dataset(test_set)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=256, shuffle=True)\n",
    "\n",
    "view_samples(trainloader, train_set.classes)\n",
    "print('Data Loading Done !')\n",
    "\n",
    "model = VisionTransformerClassifier((3, 32, 32), n_patches=8, n_blocks=6,\n",
    "                                    hidden_dim=8, n_heads=8, out_d=10).to(device)\n",
    "\n",
    "print('Model created!')\n",
    "print(model)\n",
    "print('Starting training !')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss_list, t, train_a, test_a = model_training(\n",
    "    model, criterion, optimizer, trainloader, testloader, num_epochs=5, model_name='VisionTransformer')\n",
    "plot_training_graphs(loss_list)\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    test_labels = []\n",
    "    test_output = []\n",
    "    for batch in testloader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_loss += loss.detach().cpu().item() / len(testloader)\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1)\n",
    "                             == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(f'Confusion Matrix')\n",
    "    confusionMatrixAndAccuracyReport(y.cpu(), y_hat.cpu(), test_set.classes)\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}