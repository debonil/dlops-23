{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms as T\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print('Starting ...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Available device ==> {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def model_training(model, criterion, optimizer, trainloader, testloader, num_epochs=10, model_name='model'):\n",
    "    start = time.time()\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += metrics.accuracy_score(labels.cpu().detach(\n",
    "            ).numpy(), outputs.cpu().detach().numpy().argmax(axis=1))\n",
    "        # Evaluate the model on the validation set\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += metrics.accuracy_score(labels.cpu().detach(\n",
    "                ).numpy(), outputs.cpu().detach().numpy().argmax(axis=1))\n",
    "        train_loss = train_loss/len(trainloader)\n",
    "        val_loss = val_loss/len(testloader)\n",
    "        train_acc = train_acc/len(trainloader)\n",
    "        val_acc = val_acc/len(testloader)\n",
    "        print(f'Epoch: {epoch+1} ({timeSince(start)}) \\tTraining Loss: {train_loss:.3f}, \\tTest Loss: {val_loss:.3f},  \\tTraining acc: {train_acc:.2f}, \\tTest acc: {val_acc:.2f}, ', flush=True)\n",
    "        loss_list.append([train_loss, val_loss, train_acc, val_acc])\n",
    "\n",
    "    print(\n",
    "        f'Training completed in {timeSince(start)} \\tTraining Loss: {loss_list[-1][0]:.3f}, \\tTest Loss: {loss_list[-1][1]:.3f},  \\tTraining acc: {loss_list[-1][2]:.2f}, \\tTest acc: {loss_list[-1][3]:.2f}, ')\n",
    "    return np.array(loss_list), time.time()-start, loss_list[-1][2], loss_list[-1][3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(rc={'axes.facecolor': 'lightblue', 'figure.facecolor': 'lightblue'})\n",
    "\n",
    "\n",
    "def confusionMatrixAndAccuracyReport(Y_test, Y_pred, classes, title=''):\n",
    "    cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "    overallAccuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "    classwiseAccuracy = cm.diagonal()/cm.sum(axis=1)\n",
    "\n",
    "    f1_score = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\n",
    "        f'{title} : Accuracy : {overallAccuracy*100:3.2f}% | F1 Score : {f1_score*100:3.2f}% ', size=14)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    cm.index.name = 'True Label'\n",
    "    cm.columns.name = 'Predicted Label'\n",
    "    sns.heatmap(data=cm, annot=True, square=True,  cmap='Blues',\n",
    "                fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        f'confusion_mat_{title}_{time.time()}.png', bbox_inches='tight')\n",
    "    print(f'Accuracy: {overallAccuracy*100:3.3f}%')\n",
    "    print(f'F1 Score: {f1_score*100:3.3f}%')\n",
    "    classwiseAccuracy_df = pd.DataFrame(\n",
    "        data=[classwiseAccuracy], columns=classes)\n",
    "    print(\n",
    "        f'\\nClasswise Accuracy Score: \\n{classwiseAccuracy_df.to_markdown(index=False)}')\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(cm.to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_graphs(loss_list, title=''):\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    plot = fig.add_subplot(1, 2, 1)\n",
    "    plot.set_title(f\"{title} : Training vs Validation loss\")\n",
    "    plot.plot(loss_list[:, 0], linestyle='--', label=\"Training Loss\")\n",
    "    plot.plot(loss_list[:, 1], linestyle='-', label=\"Validation Loss\")\n",
    "    plot.set_xlabel(\"Epoch\")\n",
    "    plot.set_ylabel(\"Loss\")\n",
    "    plot.legend()\n",
    "    plot = fig.add_subplot(1, 2, 2)\n",
    "    plot.set_title(f\"{title} : Training vs Validation Accuracy\")\n",
    "    plot.plot(loss_list[:, 2], linestyle='--', label=\"Training Accuracy\")\n",
    "    plot.plot(loss_list[:, 3], linestyle='-', label=\"Validation Accuracy\")\n",
    "    plot.set_xlabel(\"Epoch\")\n",
    "    plot.set_ylabel(\"Accuracy\")\n",
    "    plot.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        f'training_loss_{title}_{time.time()}.png', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_patches_from_images(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i + 1) * patch_size,\n",
    "                              j * patch_size: (j + 1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, d, n_heads=2, activation_fn=torch.nn.Softmax):\n",
    "        super(MultiHeadSelfAttentionBlock, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.activation_fn = activation_fn(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.activation_fn(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "\n",
    "class VisionTransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, mlp_ratio=4, mlp_activation=torch.nn.GELU, msa_activation=torch.nn.Softmax):\n",
    "        super(VisionTransformerBlock, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.mhsa = MultiHeadSelfAttentionBlock(\n",
    "            hidden_dim, n_heads, msa_activation)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, mlp_ratio * hidden_dim),\n",
    "            mlp_activation(),\n",
    "            torch.nn.Linear(mlp_ratio * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.pe = torch.nn.Parameter(torch.zeros(max_len, d_model))\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class VisionTransformerClassifier(torch.nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_dim=8, n_heads=2, out_d=10, activation_fn=torch.nn.ReLU, learn_pos_emb=False):\n",
    "        # Super constructor\n",
    "        super(VisionTransformerClassifier, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.chw = chw  # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_activation = torch.nn.GELU if activation_fn == None else activation_fn\n",
    "        self.msa_activation = torch.nn.Softmax if activation_fn == None else activation_fn\n",
    "        self.learn_pos_emb = learn_pos_emb\n",
    "\n",
    "        # Input and patches sizes\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_dim)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = torch.nn.Parameter(torch.rand(1, self.hidden_dim))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        if learn_pos_emb:\n",
    "            self.positional_embeddings = PositionalEmbedding(\n",
    "                hidden_dim, n_patches ** 2 + 1)\n",
    "        else:\n",
    "            self.register_buffer('positional_embeddings', get_positional_embeddings(\n",
    "                n_patches ** 2 + 1, hidden_dim), persistent=False)\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "            [VisionTransformerBlock(hidden_dim, n_heads, mlp_activation=activation_fn) for _ in range(n_blocks)])\n",
    "\n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, out_d),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = split_patches_from_images(images, self.n_patches).to(\n",
    "            self.positional_embeddings.device)\n",
    "\n",
    "        # Running linear layer tokenization\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        # Map to output dimension, output category distribution\n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(\n",
    "                i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def view_samples(testloader, classes):\n",
    "\n",
    "    test_images, labels = next(iter(testloader))\n",
    "    print(test_images.shape)\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    nrows = 3\n",
    "    ncols = 10\n",
    "    b = np.random.randint(0, test_images.shape[0]-nrows*ncols)\n",
    "    for i in range(nrows*ncols):\n",
    "        plot = fig.add_subplot(nrows, ncols, i+1)\n",
    "        plot.set_title(classes[labels[i+b].cpu().numpy()])\n",
    "        plot.imshow(np.transpose(test_images[i+b], (1, 2, 0)).cpu())\n",
    "    plt.show()\n",
    "    plt.savefig(f'view_samples_{time.time()}.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "def filter_dataset(dataset_full):\n",
    "    # Selecting even classes 0,2,4,6,8 as roll number is odd (M21AIE225)\n",
    "    targets = np.array(dataset_full.targets)\n",
    "    idx = (targets == 0) | (targets == 2) | (\n",
    "        targets == 4) | (targets == 6) | (targets == 8)\n",
    "    dataset_full.targets = np.rint(targets[idx]/2).astype(int)\n",
    "    dataset_full.data = dataset_full.data[idx]\n",
    "    dataset_full.classes = [dataset_full.classes[c] for c in [0, 2, 4, 6, 8]]\n",
    "    return dataset_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Loading ...')\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_set = filter_dataset(train_set)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "test_set = filter_dataset(test_set)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "view_samples(trainloader, train_set.classes)\n",
    "print('Data Loading Done !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activation_fns = [torch.nn.ReLU, torch.nn.Tanh, torch.nn.GELU]\n",
    "\n",
    "positional_embeddings = [\n",
    "    False,  # for cosine positional embedding\n",
    "    True,  # for learnable positional embedding\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for pe in positional_embeddings:\n",
    "    for act_fn in activation_fns:\n",
    "        print(\n",
    "            f'\\n\\n***\\t\\t\\t\\tActivation Function = {act_fn.__name__}\\tLearnable Posisional Embedding = {pe}\\t\\t\\t\\t***\\n\\n')\n",
    "\n",
    "        n_patches = 8\n",
    "        n_blocks = 6\n",
    "        hidden_dim = 8\n",
    "        n_heads = 8\n",
    "\n",
    "        if pe:\n",
    "            n_blocks = 4\n",
    "            n_heads = 6\n",
    "            hidden_dim = 12\n",
    "        else:\n",
    "            n_blocks = 6\n",
    "            n_heads = 8\n",
    "            hidden_dim = 8\n",
    "\n",
    "        model = VisionTransformerClassifier((3, 32, 32), n_patches=n_patches, n_blocks=n_blocks,\n",
    "                                            hidden_dim=hidden_dim, n_heads=n_heads, out_d=5, activation_fn=act_fn).to(device)\n",
    "\n",
    "        print(\n",
    "            f'Model created with parameter  n_blocks = { n_blocks }, n_heads = { n_heads }, hidden_dim = { hidden_dim }, act_fn = { act_fn.__name__ }, positional embd = { \"Learnable\" if pe else \"Cosine\" }, ')\n",
    "        # print(model)\n",
    "        model_name = f'ViT_{act_fn.__name__}_{ \"Learnable\" if pe else \"Cosine\" }_{n_blocks}B{n_heads}H{hidden_dim}D'\n",
    "        print(f'Starting training {model_name}!')\n",
    "        lr = 1e-4\n",
    "        epoch = 10\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        print(f'optimizer => {optimizer}')\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        print(f'criterion => {criterion}')\n",
    "        print(f'learning rate => {lr}')\n",
    "        print(f'epoch => {epoch}')\n",
    "        loss_list, t, train_a, test_a = model_training(\n",
    "            model, criterion, optimizer, trainloader, testloader, num_epochs=epoch, model_name='VisionTransformer')\n",
    "        plot_training_graphs(loss_list, title=model_name)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_labels = []\n",
    "            test_output = []\n",
    "            for batch in testloader:\n",
    "                x, y = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x)\n",
    "                test_labels += y.cpu()\n",
    "                test_output += torch.argmax(y_hat, dim=1).cpu()\n",
    "\n",
    "            test_labels = np.array(test_labels)\n",
    "            test_output = np.array(test_output)\n",
    "            print(f'\\nModel Evaluation Summary:')\n",
    "            confusionMatrixAndAccuracyReport(\n",
    "                test_labels, test_output, test_set.classes, title=model_name)\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"Positional Embed\": 'Learnable PE' if pe else 'Cosine PE',\n",
    "            \"Activation Fn\": act_fn.__name__,\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training Time\": t,\n",
    "            \"Training Accuracy\": train_a,\n",
    "            \"Test Accuracy\": test_a,\n",
    "        })\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(\"M21AIE225_Ass_3_Q1_resuts_1.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\\n Overall Summary : \\n\")\n",
    "print(df.to_markdown(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
