{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9NEVKMz0v5s"
      },
      "source": [
        "<!-- ---\n",
        "title: Distributed Training on CPUs, GPUs or TPUs\n",
        "weight: 1\n",
        "date: 2021-09-18\n",
        "downloads: true\n",
        "tags:\n",
        "  - multi GPUs on a single node\n",
        "  - multi GPUs on multiple nodes\n",
        "  - TPUs on Colab\n",
        "  - Jupyter Notebooks\n",
        "--- -->\n",
        "# Distributed Training with Ignite on CIFAR10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHmvDGFx10HT"
      },
      "source": [
        "This tutorial is a brief introduction on how you can do distributed training with Ignite on one or more CPUs, GPUs or TPUs. We will also introduce several helper functions and Ignite concepts (setup common training handlers, save to/ load from checkpoints, etc.) which you can easily incorporate in your code.\n",
        "\n",
        "<!--more-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trJ7_a7f17pg"
      },
      "source": [
        "We will use distributed training to train a predefined [ResNet18](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet18) on [CIFAR10](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10) using either of the following configurations:\n",
        "\n",
        "* Single Node, One or More GPUs\n",
        "* Multiple Nodes, Multiple GPUs\n",
        "* Single Node, Multiple CPUs\n",
        "* TPUs on Google Colab\n",
        "* On Jupyter Notebooks\n",
        "\n",
        "The type of distributed training we will use is called data parallelism in which we:\n",
        "\n",
        ">   1. Copy the model on each GPU\n",
        ">   2. Split the dataset and fit the models on different subsets\n",
        ">   3. Communicate the gradients at each iteration to keep the models in sync\n",
        ">\n",
        "> -- <cite>[Distributed Deep Learning 101: Introduction](https://towardsdatascience.com/distributed-deep-learning-101-introduction-ebfc1bcd59d9)</cite>\n",
        "\n",
        "PyTorch provides a [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) API for this task however the implementation that supports different backends + configurations is tedious. In this example, we will see how to can enable data distributed training which is adaptable to various backends in just a few lines of code alongwith:\n",
        "* Computing training and validation metrics\n",
        "* Setup logging (and connecting with ClearML)\n",
        "* Saving the best model weights\n",
        "* Setting LR Scheduler\n",
        "* Using Automatic Mixed Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLrQ6EH4uoD"
      },
      "source": [
        "## Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7k6WVw5_uts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffeed59-bd13-40b9-8e48-f4e9bc66214a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.5/266.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytorch-ignite) (23.0)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.9/dist-packages (from pytorch-ignite) (2.0.0+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<3,>=1.3->pytorch-ignite) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<3,>=1.3->pytorch-ignite) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.11\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcvTTo1s8Huq"
      },
      "source": [
        "### For parsing arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIgDDJbS8Fy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593dd814-27ae-42fb-fe90-d2a4020ccc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire) (2.2.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=7ed6a87a1bb57d5431796665db8b2b2738721af0f12ae4cf5087eda9d4b426ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09xlncFZfmeS"
      },
      "source": [
        "## Download Data\n",
        "\n",
        "Let's download our data first which can later be used by all the processes to instantiate our dataloaders. The following command will download the CIFAR10 dataset to a folder `cifar10`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRpACWtUfyn1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384215fb-93fe-4788-af33-bec5541376e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:01<00:00, 89775284.73it/s]\n",
            "Extracting cifar10/cifar-10-python.tar.gz to cifar10\n"
          ]
        }
      ],
      "source": [
        "!python -c \"from torchvision.datasets import CIFAR10; CIFAR10('cifar10', download=True)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D71VkD74he9J"
      },
      "source": [
        "## Common Configuration\n",
        "\n",
        "We maintain a `config` dictionary which can be extended or changed to store parameters required during training. We can refer back to this code when we will use these parameters later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T16:59:33.061456Z",
          "iopub.status.busy": "2021-09-14T16:59:33.061157Z",
          "iopub.status.idle": "2021-09-14T16:59:33.069066Z",
          "shell.execute_reply": "2021-09-14T16:59:33.068110Z",
          "shell.execute_reply.started": "2021-09-14T16:59:33.061424Z"
        },
        "id": "9bg7unvqhegL"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"seed\": 543,\n",
        "    \"data_path\": \"cifar10\",\n",
        "    \"output_path\": \"output-cifar10/\",\n",
        "    \"model\": \"resnet18\",\n",
        "    \"batch_size\": 512,\n",
        "    \"momentum\": 0.9,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"num_workers\": 2,\n",
        "    \"num_epochs\": 5,\n",
        "    \"learning_rate\": 0.4,\n",
        "    \"num_warmup_epochs\": 1,\n",
        "    \"validate_every\": 3,\n",
        "    \"checkpoint_every\": 200,\n",
        "    \"backend\": None,\n",
        "    \"resume_from\": None,\n",
        "    \"log_every_iters\": 15,\n",
        "    \"nproc_per_node\": None,\n",
        "    \"with_clearml\": False,\n",
        "    \"with_amp\": False,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzuG8QAr5Djf"
      },
      "source": [
        "## Basic Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIgzky7Q7kUk"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T16:59:34.353096Z",
          "iopub.status.busy": "2021-09-14T16:59:34.352425Z",
          "iopub.status.idle": "2021-09-14T17:00:15.082104Z",
          "shell.execute_reply": "2021-09-14T17:00:15.080743Z",
          "shell.execute_reply.started": "2021-09-14T16:59:34.353037Z"
        },
        "id": "0RVISbXd_h1F"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    Pad,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "import ignite\n",
        "import ignite.distributed as idist\n",
        "from ignite.contrib.engines import common\n",
        "from ignite.handlers import PiecewiseLinear\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.handlers import Checkpoint, global_step_from_engine\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from ignite.utils import manual_seed, setup_logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVXiYInWikTn"
      },
      "source": [
        "Next we will take the help of `auto_` methods in `idist` ([`ignite.distributed`](https://pytorch.org/ignite/distributed.html#)) to make our dataloaders, model and optimizer automatically adapt to the current configuration `backend=None` (non-distributed) or for backends like `nccl`, `gloo`, and `xla-tpu` (distributed).\n",
        "\n",
        "Note that we are free to partially use or not use `auto_` methods at all and instead can implement something custom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r83mIS4QCdg"
      },
      "source": [
        "### Dataloaders\n",
        "\n",
        "Next we are going to instantiate the train and test datasets from `data_path`, apply transforms to it and return them via `get_train_test_datasets()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:17.777175Z",
          "iopub.status.busy": "2021-09-14T17:01:17.776871Z",
          "iopub.status.idle": "2021-09-14T17:01:17.785864Z",
          "shell.execute_reply": "2021-09-14T17:01:17.784669Z",
          "shell.execute_reply.started": "2021-09-14T17:01:17.777146Z"
        },
        "id": "Y3BKYL7XGpZL"
      },
      "outputs": [],
      "source": [
        "def get_train_test_datasets(path):\n",
        "    train_transform = Compose(\n",
        "        [\n",
        "            Pad(4),\n",
        "            RandomCrop(32, fill=128),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ]\n",
        "    )\n",
        "    test_transform = Compose(\n",
        "        [\n",
        "            ToTensor(),\n",
        "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_ds = datasets.CIFAR10(\n",
        "        root=path, train=True, download=False, transform=train_transform\n",
        "    )\n",
        "    test_ds = datasets.CIFAR10(\n",
        "        root=path, train=False, download=False, transform=test_transform\n",
        "    )\n",
        "\n",
        "    return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GkEcAYiRbgO"
      },
      "source": [
        "Finally, we pass the datasets to [`auto_dataloader()`](https://pytorch.org/ignite/generated/ignite.distributed.auto.auto_dataloader.html#ignite.distributed.auto.auto_dataloader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:18.697479Z",
          "iopub.status.busy": "2021-09-14T17:01:18.697159Z",
          "iopub.status.idle": "2021-09-14T17:01:18.704779Z",
          "shell.execute_reply": "2021-09-14T17:01:18.703467Z",
          "shell.execute_reply.started": "2021-09-14T17:01:18.697445Z"
        },
        "id": "7rNV-UDwRPtO"
      },
      "outputs": [],
      "source": [
        "def get_dataflow(config):\n",
        "    train_dataset, test_dataset = get_train_test_datasets(config[\"data_path\"])\n",
        "\n",
        "    train_loader = idist.auto_dataloader(\n",
        "        train_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = idist.auto_dataloader(\n",
        "        test_dataset,\n",
        "        batch_size=2 * config[\"batch_size\"],\n",
        "        num_workers=config[\"num_workers\"],\n",
        "        shuffle=False,\n",
        "    )\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNLvDK-cS2sH"
      },
      "source": [
        "### Model\n",
        "\n",
        "We check if the model given in `config` is present in [torchvision.models](https://pytorch.org/vision/stable/models.html), change the last layer to output 10 classes (as present in CIFAR10) and pass it to [`auto_model()`](https://pytorch.org/ignite/generated/ignite.distributed.auto.auto_model.html#auto-model) which makes it automatically adaptable for non-distributed and distributed configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:18.782444Z",
          "iopub.status.busy": "2021-09-14T17:01:18.782146Z",
          "iopub.status.idle": "2021-09-14T17:01:18.789078Z",
          "shell.execute_reply": "2021-09-14T17:01:18.787728Z",
          "shell.execute_reply.started": "2021-09-14T17:01:18.782416Z"
        },
        "id": "toShlIcW5oFd"
      },
      "outputs": [],
      "source": [
        "def get_model(config):\n",
        "    model_name = config[\"model\"]\n",
        "    if model_name in models.__dict__:\n",
        "        fn = models.__dict__[model_name]\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unknown model name {model_name}\")\n",
        "\n",
        "    model = idist.auto_model(fn(num_classes=10))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V14TfyCT8jQW"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "Then we can setup the optimizer using hyperameters from `config` and pass it through [`auto_optim()`](https://pytorch.org/ignite/generated/ignite.distributed.auto.auto_optim.html#ignite.distributed.auto.auto_optim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:18.842622Z",
          "iopub.status.busy": "2021-09-14T17:01:18.842285Z",
          "iopub.status.idle": "2021-09-14T17:01:18.849476Z",
          "shell.execute_reply": "2021-09-14T17:01:18.848516Z",
          "shell.execute_reply.started": "2021-09-14T17:01:18.842592Z"
        },
        "id": "Iddv29eh5qU9"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(config, model):\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=config[\"learning_rate\"],\n",
        "        momentum=config[\"momentum\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "        nesterov=True,\n",
        "    )\n",
        "    optimizer = idist.auto_optim(optimizer)\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI0o7hgr8l9q"
      },
      "source": [
        "### Criterion\n",
        "\n",
        "We put the loss function on `device`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:19.324780Z",
          "iopub.status.busy": "2021-09-14T17:01:19.324452Z",
          "iopub.status.idle": "2021-09-14T17:01:19.329773Z",
          "shell.execute_reply": "2021-09-14T17:01:19.328546Z",
          "shell.execute_reply.started": "2021-09-14T17:01:19.324748Z"
        },
        "id": "DVDKkYqS5siE"
      },
      "outputs": [],
      "source": [
        "def get_criterion():\n",
        "    return nn.CrossEntropyLoss().to(idist.device())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9No0Ockx8oRC"
      },
      "source": [
        "### LR Scheduler\n",
        "\n",
        "We will use [PiecewiseLinear](https://pytorch.org/ignite/generated/ignite.handlers.param_scheduler.PiecewiseLinear.html#ignite.handlers.param_scheduler.PiecewiseLinear) which is one of the [various LR Schedulers](https://pytorch.org/ignite/handlers.html#parameter-scheduler) Ignite provides.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:19.737913Z",
          "iopub.status.busy": "2021-09-14T17:01:19.737620Z",
          "iopub.status.idle": "2021-09-14T17:01:19.746210Z",
          "shell.execute_reply": "2021-09-14T17:01:19.744390Z",
          "shell.execute_reply.started": "2021-09-14T17:01:19.737884Z"
        },
        "id": "UcYuVeYic_e7"
      },
      "outputs": [],
      "source": [
        "def get_lr_scheduler(config, optimizer):\n",
        "    milestones_values = [\n",
        "        (0, 0.0),\n",
        "        (config[\"num_iters_per_epoch\"] * config[\"num_warmup_epochs\"], config[\"learning_rate\"]),\n",
        "        (config[\"num_iters_per_epoch\"] * config[\"num_epochs\"], 0.0),\n",
        "    ]\n",
        "    lr_scheduler = PiecewiseLinear(\n",
        "        optimizer, param_name=\"lr\", milestones_values=milestones_values\n",
        "    )\n",
        "    return lr_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjVYZdn49PKD"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp6sWINP9CAq"
      },
      "source": [
        "### Save Models\n",
        "\n",
        "We can create checkpoints using either a handler (in case of ClearML) or by simply passing the path of the checkpoint file to `save_handler`:\n",
        "If specified `with-clearml=True`, we will save the models in ClearML's File Server using [`ClearMLSaver()`](https://pytorch.org/ignite/generated/ignite.contrib.handlers.clearml_logger.html#ignite.contrib.handlers.clearml_logger.ClearMLSaver)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:20.256676Z",
          "iopub.status.busy": "2021-09-14T17:01:20.256397Z",
          "iopub.status.idle": "2021-09-14T17:01:20.262069Z",
          "shell.execute_reply": "2021-09-14T17:01:20.261000Z",
          "shell.execute_reply.started": "2021-09-14T17:01:20.256647Z"
        },
        "id": "-DG0Pj4pJJFw"
      },
      "outputs": [],
      "source": [
        "def get_save_handler(config):\n",
        "    if config[\"with_clearml\"]:\n",
        "        from ignite.contrib.handlers.clearml_logger import ClearMLSaver\n",
        "\n",
        "        return ClearMLSaver(dirname=config[\"output_path\"])\n",
        "\n",
        "    return config[\"output_path\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1N1iR2f9R0n"
      },
      "source": [
        "### Resume from Checkpoint\n",
        "\n",
        "If a checkpoint file path is provided, we can resume training from there by loading the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:20.613959Z",
          "iopub.status.busy": "2021-09-14T17:01:20.613673Z",
          "iopub.status.idle": "2021-09-14T17:01:20.620012Z",
          "shell.execute_reply": "2021-09-14T17:01:20.618881Z",
          "shell.execute_reply.started": "2021-09-14T17:01:20.613923Z"
        },
        "id": "j9La55z97PVa"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(resume_from):\n",
        "    checkpoint_fp = Path(resume_from)\n",
        "    assert (\n",
        "        checkpoint_fp.exists()\n",
        "    ), f\"Checkpoint '{checkpoint_fp.as_posix()}' is not found\"\n",
        "    checkpoint = torch.load(checkpoint_fp.as_posix(), map_location=\"cpu\")\n",
        "    return checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBd6WDVE_KmO"
      },
      "source": [
        "### Create Trainer\n",
        "\n",
        "Finally, we can create our `trainer` in four steps:\n",
        "1. Create a `trainer` object using [`create_supervised_trainer()`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html#ignite.engine.create_supervised_trainer) which internally defines the steps taken to process a single batch:\n",
        "  1. Move the batch to `device` used in current distributed configuration.\n",
        "  2. Put `model` in `train()` mode.\n",
        "  3. Perform forward pass by passing the inputs through the `model` and calculating `loss`. If AMP is enabled then this step happens with [`autocast`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast) on which allows this step to run in mixed precision.\n",
        "  4. Perform backward pass. If [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html) (AMP) is enabled (speeds up computations on large neural networks and reduces memory usage while retaining performance), then the losses will be [scaled](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.scale) before calling `backward()`, `step()` the optimizer while discarding batches that contain NaNs and [update()](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.update) the scale for the next iteration.\n",
        "  5. Store the loss as `batch loss` in `state.output`.\n",
        "\n",
        "Internally, the above steps to create the `trainer` would look like:\n",
        "  ```python\n",
        "  def train_step(engine, batch):\n",
        "\n",
        "        x, y = batch[0], batch[1]\n",
        "        if x.device != device:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        with autocast(enabled=with_amp):\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()  # If with_amp=False, this is equivalent to loss.backward()\n",
        "        scaler.step(optimizer)  # If with_amp=False, this is equivalent to optimizer.step()\n",
        "        scaler.update()  # If with_amp=False, this step does nothing\n",
        "\n",
        "        return {\"batch loss\": loss.item()}\n",
        "\n",
        "  trainer = Engine(train_step)\n",
        "  ```\n",
        "3. Setup some common Ignite training handlers. You can do this individually or use [setup_common_training_handlers()](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.common.setup_common_training_handlers) that takes the `trainer` and a subset of the dataset (`train_sampler`) alongwith:\n",
        "  * A dictionary mapping on what to save in the checkpoint (`to_save`) and how often (`save_every_iters`).\n",
        "  * The LR Scheduler\n",
        "  * The output of `train_step()`\n",
        "  * Other handlers\n",
        "4. If `resume_from` file path is provided, load the states of objects `to_save` from the checkpoint file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:21.143122Z",
          "iopub.status.busy": "2021-09-14T17:01:21.142824Z",
          "iopub.status.idle": "2021-09-14T17:01:21.153455Z",
          "shell.execute_reply": "2021-09-14T17:01:21.152265Z",
          "shell.execute_reply.started": "2021-09-14T17:01:21.143089Z"
        },
        "id": "ptmfSvESbEPE"
      },
      "outputs": [],
      "source": [
        "def create_trainer(\n",
        "    model, optimizer, criterion, lr_scheduler, train_sampler, config, logger\n",
        "):\n",
        "\n",
        "    device = idist.device()\n",
        "    amp_mode = None\n",
        "    scaler = False\n",
        "        \n",
        "    trainer = create_supervised_trainer(\n",
        "        model,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        device=device,\n",
        "        non_blocking=True,\n",
        "        output_transform=lambda x, y, y_pred, loss: {\"batch loss\": loss.item()},\n",
        "        amp_mode=\"amp\" if config[\"with_amp\"] else None,\n",
        "        scaler=config[\"with_amp\"],\n",
        "    )\n",
        "    trainer.logger = logger\n",
        "\n",
        "    to_save = {\n",
        "        \"trainer\": trainer,\n",
        "        \"model\": model,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "    }\n",
        "    metric_names = [\n",
        "        \"batch loss\",\n",
        "    ]\n",
        "\n",
        "    common.setup_common_training_handlers(\n",
        "        trainer=trainer,\n",
        "        train_sampler=train_sampler,\n",
        "        to_save=to_save,\n",
        "        save_every_iters=config[\"checkpoint_every\"],\n",
        "        save_handler=get_save_handler(config),\n",
        "        lr_scheduler=lr_scheduler,\n",
        "        output_names=metric_names if config[\"log_every_iters\"] > 0 else None,\n",
        "        with_pbars=False,\n",
        "        clear_cuda_cache=False,\n",
        "    )\n",
        "\n",
        "    if config[\"resume_from\"] is not None:\n",
        "        checkpoint = load_checkpoint(config[\"resume_from\"])\n",
        "        Checkpoint.load_objects(to_load=to_save, checkpoint=checkpoint)\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ius6EM9aiG"
      },
      "source": [
        "## Evaluator\n",
        "\n",
        "The evaluator will be created via [`create_supervised_evaluator()`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_evaluator.html#ignite.engine.create_supervised_evaluator) which internally will:\n",
        "1. Set the `model` to `eval()` mode.\n",
        "2. Move the batch to `device` used in current distributed configuration.\n",
        "3. Perform forward pass. If AMP is enabled, `autocast` will be on.\n",
        "4. Store the predictions and labels in `state.output` to compute metrics.\n",
        "\n",
        "It will also attach the Ignite metrics passed to the `evaluator`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:21.765740Z",
          "iopub.status.busy": "2021-09-14T17:01:21.765110Z",
          "iopub.status.idle": "2021-09-14T17:01:21.771498Z",
          "shell.execute_reply": "2021-09-14T17:01:21.770611Z",
          "shell.execute_reply.started": "2021-09-14T17:01:21.765695Z"
        },
        "id": "JPVzU6CqNlE4"
      },
      "outputs": [],
      "source": [
        "def create_evaluator(model, metrics, config):\n",
        "    device = idist.device()\n",
        "\n",
        "    amp_mode = \"amp\" if config[\"with_amp\"] else None\n",
        "    evaluator = create_supervised_evaluator(\n",
        "        model, metrics=metrics, device=device, non_blocking=True, amp_mode=amp_mode\n",
        "    )\n",
        "    \n",
        "    return evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opzRc2DR9dz-"
      },
      "source": [
        "## Training\n",
        "\n",
        "Before we begin training, we must setup a few things on the master process (`rank` = 0):\n",
        "* Create folder to store checkpoints, best models and output of tensorboard logging in the format - model_backend_rank_time.\n",
        "* If ClearML FileServer is used to save models, then a `Task` has to be created, and we pass our `config` dictionary and the specific hyper parameters that are part of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:22.274841Z",
          "iopub.status.busy": "2021-09-14T17:01:22.273903Z",
          "iopub.status.idle": "2021-09-14T17:01:22.283701Z",
          "shell.execute_reply": "2021-09-14T17:01:22.283030Z",
          "shell.execute_reply.started": "2021-09-14T17:01:22.274775Z"
        },
        "id": "3_ahtRZ_i-k8"
      },
      "outputs": [],
      "source": [
        "def setup_rank_zero(logger, config):\n",
        "    device = idist.device()\n",
        "\n",
        "    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    output_path = config[\"output_path\"]\n",
        "    folder_name = (\n",
        "        f\"{config['model']}_backend-{idist.backend()}-{idist.get_world_size()}_{now}\"\n",
        "    )\n",
        "    output_path = Path(output_path) / folder_name\n",
        "    if not output_path.exists():\n",
        "        output_path.mkdir(parents=True)\n",
        "    config[\"output_path\"] = output_path.as_posix()\n",
        "    logger.info(f\"Output path: {config['output_path']}\")\n",
        "\n",
        "    if config[\"with_clearml\"]:\n",
        "        from clearml import Task\n",
        "\n",
        "        task = Task.init(\"CIFAR10-Training\", task_name=output_path.stem)\n",
        "        task.connect_configuration(config)\n",
        "        # Log hyper parameters\n",
        "        hyper_params = [\n",
        "            \"model\",\n",
        "            \"batch_size\",\n",
        "            \"momentum\",\n",
        "            \"weight_decay\",\n",
        "            \"num_epochs\",\n",
        "            \"learning_rate\",\n",
        "            \"num_warmup_epochs\",\n",
        "        ]\n",
        "        task.connect({k: v for k, v in config.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXpew9g3PUKI"
      },
      "source": [
        "### Logging\n",
        "\n",
        "This step is optional, however, we can pass a [`setup_logger()`](https://pytorch.org/ignite/utils.html#ignite.utils.setup_logger) object to `log_basic_info()` and log all basic information such as different versions, current configuration, `device` and `backend` used by the current process (identified by its local rank), and number of processes (world size). `idist` (`ignite.distributed`) provides several utility functions like [`get_local_rank()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.get_local_rank), [`backend()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.backend), [`get_world_size()`](https://pytorch.org/ignite/distributed.html#ignite.distributed.utils.get_world_size), etc. to make this possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:22.883100Z",
          "iopub.status.busy": "2021-09-14T17:01:22.882657Z",
          "iopub.status.idle": "2021-09-14T17:01:22.891892Z",
          "shell.execute_reply": "2021-09-14T17:01:22.891225Z",
          "shell.execute_reply.started": "2021-09-14T17:01:22.883044Z"
        },
        "id": "g61dzVvnEVvG"
      },
      "outputs": [],
      "source": [
        "def log_basic_info(logger, config):\n",
        "    logger.info(f\"Train on CIFAR10\")\n",
        "    logger.info(f\"- PyTorch version: {torch.__version__}\")\n",
        "    logger.info(f\"- Ignite version: {ignite.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        # explicitly import cudnn as torch.backends.cudnn can not be pickled with hvd spawning procs\n",
        "        from torch.backends import cudnn\n",
        "\n",
        "        logger.info(\n",
        "            f\"- GPU Device: {torch.cuda.get_device_name(idist.get_local_rank())}\"\n",
        "        )\n",
        "        logger.info(f\"- CUDA version: {torch.version.cuda}\")\n",
        "        logger.info(f\"- CUDNN version: {cudnn.version()}\")\n",
        "\n",
        "    logger.info(\"\\n\")\n",
        "    logger.info(\"Configuration:\")\n",
        "    for key, value in config.items():\n",
        "        logger.info(f\"\\t{key}: {value}\")\n",
        "    logger.info(\"\\n\")\n",
        "\n",
        "    if idist.get_world_size() > 1:\n",
        "        logger.info(\"\\nDistributed setting:\")\n",
        "        logger.info(f\"\\tbackend: {idist.backend()}\")\n",
        "        logger.info(f\"\\tworld size: {idist.get_world_size()}\")\n",
        "        logger.info(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QT7tiAcJEk0"
      },
      "source": [
        "This is a standard utility function to log `train` and `val` metrics after `validate_every` epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:23.571959Z",
          "iopub.status.busy": "2021-09-14T17:01:23.571522Z",
          "iopub.status.idle": "2021-09-14T17:01:23.578820Z",
          "shell.execute_reply": "2021-09-14T17:01:23.577344Z",
          "shell.execute_reply.started": "2021-09-14T17:01:23.571919Z"
        },
        "id": "9hqmFjOJN8kK"
      },
      "outputs": [],
      "source": [
        "def log_metrics(logger, epoch, elapsed, tag, metrics):\n",
        "    metrics_output = \"\\n\".join([f\"\\t{k}: {v}\" for k, v in metrics.items()])\n",
        "    logger.info(\n",
        "        f\"\\nEpoch {epoch} - Evaluation time (seconds): {elapsed:.2f} - {tag} metrics:\\n {metrics_output}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKzNAa1JVRR"
      },
      "source": [
        "### Begin Training\n",
        "\n",
        "This is where the main logic resides, i.e. we will call all the above functions from within here:\n",
        "1. Basic Setup\n",
        "  1. We set a [`manual_seed()`](https://pytorch.org/ignite/utils.html#ignite.utils.manual_seed) and [`setup_logger()`](https://pytorch.org/ignite/utils.html#ignite.utils.setup_logger), then log all basic information.\n",
        "  2. Initialise `dataloaders`, `model`, `optimizer`, `criterion` and `lr_scheduler`.\n",
        "2. We use the above objects to create a `trainer`.\n",
        "3. Evaluator\n",
        "  1. Define some relevant Ignite metrics like [`Accuracy()`](https://pytorch.org/ignite/generated/ignite.metrics.Accuracy.html#accuracy) and [`Loss()`](https://pytorch.org/ignite/generated/ignite.metrics.Loss.html#loss).\n",
        "  2. Create two evaluators: `train_evaluator` and `val_evaluator` to compute metrics on the `train_dataloader` and `val_dataloader` respectively, however `val_evaluator` will store the best models based on validation metrics.\n",
        "  3. Define `run_validation()` to compute metrics on both dataloaders and log them. Then we attach this function to `trainer` to run after `validate_every` epochs and after training is complete.\n",
        "4. Setup TensorBoard logging using [`setup_tb_logging()`](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.common.setup_tb_logging) on the master process for the trainer and evaluators so that training and validation metrics along with the learning rate can be logged.\n",
        "5. Define a [`Checkpoint()`](https://pytorch.org/ignite/generated/ignite.handlers.checkpoint.Checkpoint.html#ignite.handlers.checkpoint.Checkpoint) object to store the two best models (`n_saved`) by validation accuracy (defined in `metrics` as `Accuracy()`) and attach it to `val_evaluator` so that it can be executed everytime `val_evaluator` runs.\n",
        "6. Try training on `train_loader` for `num_epochs`\n",
        "7. Close Tensorboard logger once training is completed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-14T17:01:24.568735Z",
          "iopub.status.busy": "2021-09-14T17:01:24.568265Z",
          "iopub.status.idle": "2021-09-14T17:01:24.585423Z",
          "shell.execute_reply": "2021-09-14T17:01:24.584535Z",
          "shell.execute_reply.started": "2021-09-14T17:01:24.568684Z"
        },
        "id": "M3l1U7_vL8pg"
      },
      "outputs": [],
      "source": [
        "def training(local_rank, config):\n",
        "\n",
        "    rank = idist.get_rank()\n",
        "    manual_seed(config[\"seed\"] + rank)\n",
        "\n",
        "    logger = setup_logger(name=\"CIFAR10-Training\")\n",
        "    log_basic_info(logger, config)\n",
        "\n",
        "    if rank == 0:\n",
        "        setup_rank_zero(logger, config)\n",
        "\n",
        "    train_loader, val_loader = get_dataflow(config)\n",
        "    model = get_model(config)\n",
        "    optimizer = get_optimizer(config, model)\n",
        "    criterion = get_criterion()\n",
        "    config[\"num_iters_per_epoch\"] = len(train_loader)\n",
        "    lr_scheduler = get_lr_scheduler(config, optimizer)\n",
        "\n",
        "    trainer = create_trainer(\n",
        "        model, optimizer, criterion, lr_scheduler, train_loader.sampler, config, logger\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": Accuracy(),\n",
        "        \"Loss\": Loss(criterion),\n",
        "    }\n",
        "\n",
        "    train_evaluator = create_evaluator(model, metrics, config)\n",
        "    val_evaluator = create_evaluator(model, metrics, config)\n",
        "\n",
        "    def run_validation(engine):\n",
        "        epoch = trainer.state.epoch\n",
        "        state = train_evaluator.run(train_loader)\n",
        "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"train\", state.metrics)\n",
        "        state = val_evaluator.run(val_loader)\n",
        "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"val\", state.metrics)\n",
        "\n",
        "    trainer.add_event_handler(\n",
        "        Events.EPOCH_COMPLETED(every=config[\"validate_every\"]) | Events.COMPLETED,\n",
        "        run_validation,\n",
        "    )\n",
        "\n",
        "    if rank == 0:\n",
        "        evaluators = {\"train\": train_evaluator, \"val\": val_evaluator}\n",
        "        tb_logger = common.setup_tb_logging(\n",
        "            config[\"output_path\"], trainer, optimizer, evaluators=evaluators\n",
        "        )\n",
        "\n",
        "    best_model_handler = Checkpoint(\n",
        "        {\"model\": model},\n",
        "        get_save_handler(config),\n",
        "        filename_prefix=\"best\",\n",
        "        n_saved=2,\n",
        "        global_step_transform=global_step_from_engine(trainer),\n",
        "        score_name=\"val_accuracy\",\n",
        "        score_function=Checkpoint.get_default_score_fn(\"Accuracy\"),\n",
        "    )\n",
        "    val_evaluator.add_event_handler(\n",
        "        Events.COMPLETED,\n",
        "        best_model_handler,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.run(train_loader, max_epochs=config[\"num_epochs\"])\n",
        "    except Exception as e:\n",
        "        logger.exception(\"\")\n",
        "        raise e\n",
        "\n",
        "    if rank == 0:\n",
        "        tb_logger.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WioiRM5U9ipQ"
      },
      "source": [
        "## Running Distributed Code\n",
        "\n",
        "We can easily run the above code with the context manager [Parallel](https://pytorch.org/ignite/generated/ignite.distributed.launcher.Parallel.html#ignite.distributed.launcher.Parallel):\n",
        "\n",
        "```python\n",
        "with idist.Parallel(backend=backend, nproc_per_node=nproc_per_node) as parallel:\n",
        "    parallel.run(training, config)\n",
        "```\n",
        "`Parallel` enables us to run the same code across all supported distributed backends and non-distributed configurations in a seamless manner. Here backend refers to a distributed communication framework. Read more about which backend to choose [here](https://pytorch.org/docs/stable/distributed.html#backends). `Parallel` accepts a `backend` and either:\n",
        "\n",
        "> Spawns `nproc_per_node` child processes and initialize a processing group according to provided backend (useful for standalone scripts).\n",
        "\n",
        "This way uses `torch.multiprocessing.spawn` and is the default way to spawn processes. However, this way is slower due to initialization overhead. \n",
        "\n",
        "or\n",
        "> Only initialize a processing group given the backend (useful with tools like `torchrun`, `horovodrun`, etc).\n",
        "\n",
        "This way is recommended since training is faster and easier to extend to multiple scripts.\n",
        "\n",
        "We can pass additional information to `Parallel` collectively as `spawn_kwargs` as we will see below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJuv6mqHzpVJ"
      },
      "source": [
        "**Note:** It is recommended to run distributed code as scripts for ease of use, however we can also spawn processes in a Jupyter notebook (see end of tutorial). The complete code as a script can be found [here](https://github.com/pytorch-ignite/examples/blob/main/tutorials/intermediate/cifar10-distributed.py). Choose one of the suggested ways below to run the script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYq3RGY1t9me"
      },
      "source": [
        "## Run in Jupyter Notebook\n",
        "\n",
        "We will have to spawn processes in a notebook and therefore, we will use internal spawning to achieve that. For multiple GPUs, use `backend=nccl` and `backend=gloo` for multiple CPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DWUEfZHuTDz",
        "outputId": "925fb3d7-c934-4a64-ee5e-5755e901dd39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-15 05:11:34,281 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'\n",
            "2023-04-15 05:11:34,283 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 1\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2023-04-15 05:11:34,285 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f4c18b3bee0>' in 1 processes\n",
            "2023-04-15 05:11:39,918 CIFAR10-Training INFO: Train on CIFAR10\n",
            "2023-04-15 05:11:39,920 CIFAR10-Training INFO: - PyTorch version: 2.0.0+cu118\n",
            "2023-04-15 05:11:39,921 CIFAR10-Training INFO: - Ignite version: 0.4.11\n",
            "2023-04-15 05:11:39,927 CIFAR10-Training INFO: - GPU Device: Tesla T4\n",
            "2023-04-15 05:11:39,930 CIFAR10-Training INFO: - CUDA version: 11.8\n",
            "2023-04-15 05:11:39,950 CIFAR10-Training INFO: - CUDNN version: 8700\n",
            "2023-04-15 05:11:39,951 CIFAR10-Training INFO: \n",
            "\n",
            "2023-04-15 05:11:39,954 CIFAR10-Training INFO: Configuration:\n",
            "2023-04-15 05:11:39,956 CIFAR10-Training INFO: \tseed: 543\n",
            "2023-04-15 05:11:39,959 CIFAR10-Training INFO: \tdata_path: cifar10\n",
            "2023-04-15 05:11:39,961 CIFAR10-Training INFO: \toutput_path: output-cifar10/\n",
            "2023-04-15 05:11:39,963 CIFAR10-Training INFO: \tmodel: resnet18\n",
            "2023-04-15 05:11:39,970 CIFAR10-Training INFO: \tbatch_size: 512\n",
            "2023-04-15 05:11:39,972 CIFAR10-Training INFO: \tmomentum: 0.9\n",
            "2023-04-15 05:11:39,973 CIFAR10-Training INFO: \tweight_decay: 0.0001\n",
            "2023-04-15 05:11:39,978 CIFAR10-Training INFO: \tnum_workers: 2\n",
            "2023-04-15 05:11:39,980 CIFAR10-Training INFO: \tnum_epochs: 5\n",
            "2023-04-15 05:11:39,983 CIFAR10-Training INFO: \tlearning_rate: 0.4\n",
            "2023-04-15 05:11:39,986 CIFAR10-Training INFO: \tnum_warmup_epochs: 1\n",
            "2023-04-15 05:11:39,989 CIFAR10-Training INFO: \tvalidate_every: 3\n",
            "2023-04-15 05:11:39,991 CIFAR10-Training INFO: \tcheckpoint_every: 200\n",
            "2023-04-15 05:11:39,994 CIFAR10-Training INFO: \tbackend: nccl\n",
            "2023-04-15 05:11:39,996 CIFAR10-Training INFO: \tresume_from: None\n",
            "2023-04-15 05:11:39,997 CIFAR10-Training INFO: \tlog_every_iters: 15\n",
            "2023-04-15 05:11:39,999 CIFAR10-Training INFO: \tnproc_per_node: None\n",
            "2023-04-15 05:11:40,001 CIFAR10-Training INFO: \twith_clearml: False\n",
            "2023-04-15 05:11:40,003 CIFAR10-Training INFO: \twith_amp: False\n",
            "2023-04-15 05:11:40,005 CIFAR10-Training INFO: \n",
            "\n",
            "2023-04-15 05:11:40,007 CIFAR10-Training INFO: Output path: output-cifar10/resnet18_backend-nccl-1_20230415-051140\n",
            "2023-04-15 05:11:40,955 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': \n",
            "\t{'batch_size': 512, 'num_workers': 2, 'shuffle': True, 'drop_last': True, 'pin_memory': True}\n",
            "2023-04-15 05:11:40,957 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': \n",
            "\t{'batch_size': 1024, 'num_workers': 2, 'shuffle': False, 'pin_memory': True}\n",
            "2023-04-15 05:11:44,618 CIFAR10-Training INFO: Engine run starting with max_epochs=5.\n",
            "2023-04-15 05:12:15,292 CIFAR10-Training INFO: Epoch[1] Complete. Time taken: 00:00:30.585\n",
            "2023-04-15 05:12:35,028 CIFAR10-Training INFO: Epoch[2] Complete. Time taken: 00:00:19.734\n",
            "2023-04-15 05:13:15,616 CIFAR10-Training INFO: \n",
            "Epoch 3 - Evaluation time (seconds): 18.75 - train metrics:\n",
            " \tAccuracy: 0.5679969394329897\n",
            "\tLoss: 1.198925608212186\n",
            "2023-04-15 05:13:18,461 CIFAR10-Training INFO: \n",
            "Epoch 3 - Evaluation time (seconds): 2.78 - val metrics:\n",
            " \tAccuracy: 0.5792\n",
            "\tLoss: 1.17522705078125\n",
            "2023-04-15 05:13:18,469 CIFAR10-Training INFO: Epoch[3] Complete. Time taken: 00:00:43.436\n",
            "2023-04-15 05:13:37,907 CIFAR10-Training INFO: Epoch[4] Complete. Time taken: 00:00:19.436\n",
            "2023-04-15 05:14:00,094 CIFAR10-Training INFO: Epoch[5] Complete. Time taken: 00:00:22.185\n",
            "2023-04-15 05:14:17,832 CIFAR10-Training INFO: \n",
            "Epoch 5 - Evaluation time (seconds): 17.67 - train metrics:\n",
            " \tAccuracy: 0.6632973582474226\n",
            "\tLoss: 0.9425878623097214\n",
            "2023-04-15 05:14:20,386 CIFAR10-Training INFO: \n",
            "Epoch 5 - Evaluation time (seconds): 2.48 - val metrics:\n",
            " \tAccuracy: 0.667\n",
            "\tLoss: 0.93472158203125\n",
            "2023-04-15 05:14:20,389 CIFAR10-Training INFO: Engine run complete. Time taken: 00:02:35.769\n",
            "2023-04-15 05:14:20,721 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ],
      "source": [
        "spawn_kwargs = {}\n",
        "spawn_kwargs[\"start_method\"] = \"fork\"\n",
        "spawn_kwargs[\"nproc_per_node\"] = 1\n",
        "config[\"backend\"] = \"nccl\"\n",
        "\n",
        "with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
        "    parallel.run(training, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNIC_h9fXeKI"
      },
      "source": [
        "## Important Links\n",
        "\n",
        "1. Complete code can be found [here](https://github.com/pytorch-ignite/examples/blob/main/tutorials/intermediate/cifar10-distributed.py).\n",
        "2. Example of the logs of a ClearML experiment run on this code:\n",
        "   - [With torchrun](https://app.community.clear.ml/projects/14efa0ee4c114401bd06b7748314b465/experiments/83ebffd99a3f47f49dff1075252e3371/output/execution) \n",
        "   - [With default internal spawning](https://app.community.clear.ml/projects/14efa0ee4c114401bd06b7748314b465/experiments/c2b82ec98e8445f29044c94f7efc8215/output/execution)\n",
        "   - [On Jupyter](https://app.community.clear.ml/projects/14efa0ee4c114401bd06b7748314b465/experiments/2fedd7447b114b36af7066cdb81fddae/output/execution)\n",
        "   - [On Colab with XLA](https://app.community.clear.ml/projects/14efa0ee4c114401bd06b7748314b465/experiments/fbffb4d7f9324c57979a833a789df857/output/execution)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RdfXz_0cWZ_9",
        "b_k_0YkyX1Yn",
        "osAHgAyJWomh",
        "rnOI--qIJ0ZN",
        "b1Z4JfvJJ6bt",
        "KPUtXHI3KG9w"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}